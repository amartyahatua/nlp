{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df['sentiment']\n",
    "reviews = df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = len(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size= 399\n",
      "Test size= 100\n"
     ]
    }
   ],
   "source": [
    "train_size = int(total_len*0.8)\n",
    "test_size = total_len-train_size\n",
    "\n",
    "print('Train size=',train_size)\n",
    "print('Test size=',test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df.loc[:train_size, 'review'].values\n",
    "train_y = df.loc[:train_size, 'sentiment'].values\n",
    "\n",
    "test_x = df.loc[train_size:total_len, 'review'].values\n",
    "test_y = df.loc[train_size:total_len, 'sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review'].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([ len(s.split()) for s in reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148\n"
     ]
    }
   ],
   "source": [
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n",
    "X_train_tokens = tokenizer.texts_to_sequences(train_x)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1148, 100)         1351500   \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,364,301\n",
      "Trainable params: 1,364,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/25\n",
      " - 36s - loss: 0.6945 - acc: 0.4600 - val_loss: 0.6918 - val_acc: 0.5400\n",
      "Epoch 2/25\n",
      " - 11s - loss: 0.6924 - acc: 0.5075 - val_loss: 0.6922 - val_acc: 0.5400\n",
      "Epoch 3/25\n",
      " - 5s - loss: 0.6929 - acc: 0.5175 - val_loss: 0.6921 - val_acc: 0.5400\n",
      "Epoch 4/25\n",
      " - 39s - loss: 0.6937 - acc: 0.5025 - val_loss: 0.6913 - val_acc: 0.5400\n",
      "Epoch 5/25\n",
      " - 7s - loss: 0.6935 - acc: 0.5125 - val_loss: 0.6911 - val_acc: 0.5400\n",
      "Epoch 6/25\n",
      " - 7s - loss: 0.6935 - acc: 0.5075 - val_loss: 0.6905 - val_acc: 0.5400\n",
      "Epoch 7/25\n",
      " - 17s - loss: 0.6937 - acc: 0.5100 - val_loss: 0.6905 - val_acc: 0.5400\n",
      "Epoch 8/25\n",
      " - 6s - loss: 0.6941 - acc: 0.5100 - val_loss: 0.6905 - val_acc: 0.5400\n",
      "Epoch 9/25\n",
      " - 5s - loss: 0.6950 - acc: 0.5100 - val_loss: 0.6905 - val_acc: 0.5400\n",
      "Epoch 10/25\n",
      " - 5s - loss: 0.6936 - acc: 0.5100 - val_loss: 0.6908 - val_acc: 0.5400\n",
      "Epoch 11/25\n",
      " - 7s - loss: 0.6932 - acc: 0.5075 - val_loss: 0.6917 - val_acc: 0.5400\n",
      "Epoch 12/25\n",
      " - 15s - loss: 0.6929 - acc: 0.4975 - val_loss: 0.6925 - val_acc: 0.5400\n",
      "Epoch 13/25\n",
      " - 6s - loss: 0.6942 - acc: 0.4700 - val_loss: 0.6931 - val_acc: 0.4700\n",
      "Epoch 14/25\n",
      " - 6s - loss: 0.6929 - acc: 0.5300 - val_loss: 0.6926 - val_acc: 0.5400\n",
      "Epoch 15/25\n",
      " - 5s - loss: 0.6935 - acc: 0.4950 - val_loss: 0.6917 - val_acc: 0.5400\n",
      "Epoch 16/25\n",
      " - 5s - loss: 0.6933 - acc: 0.5100 - val_loss: 0.6909 - val_acc: 0.5400\n",
      "Epoch 17/25\n",
      " - 6s - loss: 0.6941 - acc: 0.5100 - val_loss: 0.6906 - val_acc: 0.5400\n",
      "Epoch 18/25\n",
      " - 5s - loss: 0.6950 - acc: 0.5100 - val_loss: 0.6906 - val_acc: 0.5400\n",
      "Epoch 19/25\n",
      " - 5s - loss: 0.6925 - acc: 0.5100 - val_loss: 0.6909 - val_acc: 0.5400\n",
      "Epoch 20/25\n",
      " - 5s - loss: 0.6930 - acc: 0.5125 - val_loss: 0.6913 - val_acc: 0.5400\n",
      "Epoch 21/25\n",
      " - 5s - loss: 0.6918 - acc: 0.5125 - val_loss: 0.6913 - val_acc: 0.5400\n",
      "Epoch 22/25\n",
      " - 5s - loss: 0.6942 - acc: 0.5100 - val_loss: 0.6909 - val_acc: 0.5400\n",
      "Epoch 23/25\n",
      " - 5s - loss: 0.6935 - acc: 0.5100 - val_loss: 0.6908 - val_acc: 0.5400\n",
      "Epoch 24/25\n",
      " - 5s - loss: 0.6940 - acc: 0.5100 - val_loss: 0.6910 - val_acc: 0.5400\n",
      "Epoch 25/25\n",
      " - 5s - loss: 0.6928 - acc: 0.5075 - val_loss: 0.6914 - val_acc: 0.5400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb44260588>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train')\n",
    "model.fit(X_train_pad, train_y, epochs = 25, batch_size=128, validation_data=(X_test_pad, test_y), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lines = list()\n",
    "lines = df['review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "for line in lines: \n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped  if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)\n",
    "    \n",
    "    \n",
    "print(len(review_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size =  13203\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=1, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "\n",
    "print(\"Vocab size = \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 0.9999512434005737),\n",
       " ('film', 0.9999445080757141),\n",
       " ('nt', 0.9999390840530396),\n",
       " ('great', 0.9999366402626038),\n",
       " ('well', 0.999934196472168),\n",
       " ('like', 0.9999324679374695),\n",
       " ('see', 0.9999309778213501),\n",
       " ('also', 0.9999294281005859),\n",
       " ('people', 0.9999277591705322),\n",
       " ('even', 0.9999247789382935)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "filename = 'imdb_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "embedding_index = {}\n",
    "f = open(os.path.join('', 'imdb_embedding_word2vec.txt'), encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embedding_index[word] = coefs\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens 13203\n",
      "Shape of review tensor (499, 1148)\n",
      "Shape of sentiment tensor (499,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(review_lines)\n",
    "sequences = tokenizer.texts_to_sequences(review_lines)\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Number of unique tokens\", len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment = df['sentiment'].values\n",
    "\n",
    "print(\"Shape of review tensor\", review_pad.shape)\n",
    "print(\"Shape of sentiment tensor\", sentiment.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word , i in word_index.items():\n",
    "    if(i>num_words):\n",
    "        continue \n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13204\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1148, 100)         1320400   \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,333,201\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 1,320,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_DIM, \n",
    "                            embeddings_initializer=Constant(embedding_matrix), \n",
    "                            input_length = max_length, \n",
    "                            trainable = False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT*review_pad.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train....\n",
      "Train on 400 samples, validate on 99 samples\n",
      "Epoch 1/25\n",
      " - 7s - loss: 0.6944 - acc: 0.4950 - val_loss: 0.6942 - val_acc: 0.5253\n",
      "Epoch 2/25\n",
      " - 4s - loss: 0.6915 - acc: 0.5350 - val_loss: 0.6949 - val_acc: 0.5152\n",
      "Epoch 3/25\n",
      " - 4s - loss: 0.6918 - acc: 0.5400 - val_loss: 0.6955 - val_acc: 0.4444\n",
      "Epoch 4/25\n",
      " - 4s - loss: 0.6902 - acc: 0.5175 - val_loss: 0.7002 - val_acc: 0.4747\n",
      "Epoch 5/25\n",
      " - 5s - loss: 0.6929 - acc: 0.5050 - val_loss: 0.7042 - val_acc: 0.4848\n",
      "Epoch 6/25\n",
      " - 4s - loss: 0.6949 - acc: 0.4975 - val_loss: 0.7018 - val_acc: 0.4848\n",
      "Epoch 7/25\n",
      " - 4s - loss: 0.6916 - acc: 0.5200 - val_loss: 0.7015 - val_acc: 0.4848\n",
      "Epoch 8/25\n",
      " - 4s - loss: 0.6833 - acc: 0.5450 - val_loss: 0.7011 - val_acc: 0.4949\n",
      "Epoch 9/25\n",
      " - 4s - loss: 0.6935 - acc: 0.5175 - val_loss: 0.6996 - val_acc: 0.4747\n",
      "Epoch 10/25\n",
      " - 4s - loss: 0.6904 - acc: 0.5650 - val_loss: 0.6984 - val_acc: 0.4646\n",
      "Epoch 11/25\n",
      " - 4s - loss: 0.6870 - acc: 0.5625 - val_loss: 0.6985 - val_acc: 0.4747\n",
      "Epoch 12/25\n",
      " - 4s - loss: 0.6907 - acc: 0.5450 - val_loss: 0.6992 - val_acc: 0.4646\n",
      "Epoch 13/25\n",
      " - 4s - loss: 0.6884 - acc: 0.5350 - val_loss: 0.6996 - val_acc: 0.4646\n",
      "Epoch 14/25\n",
      " - 4s - loss: 0.6905 - acc: 0.5525 - val_loss: 0.7002 - val_acc: 0.4545\n",
      "Epoch 15/25\n",
      " - 4s - loss: 0.6861 - acc: 0.5750 - val_loss: 0.7033 - val_acc: 0.4444\n",
      "Epoch 16/25\n",
      " - 4s - loss: 0.6862 - acc: 0.5875 - val_loss: 0.7060 - val_acc: 0.4848\n",
      "Epoch 17/25\n",
      " - 4s - loss: 0.6872 - acc: 0.5425 - val_loss: 0.7054 - val_acc: 0.4646\n",
      "Epoch 18/25\n",
      " - 4s - loss: 0.6862 - acc: 0.5875 - val_loss: 0.7028 - val_acc: 0.4646\n",
      "Epoch 19/25\n",
      " - 4s - loss: 0.6884 - acc: 0.5525 - val_loss: 0.7033 - val_acc: 0.4848\n",
      "Epoch 20/25\n",
      " - 4s - loss: 0.6904 - acc: 0.5450 - val_loss: 0.7040 - val_acc: 0.4848\n",
      "Epoch 21/25\n",
      " - 4s - loss: 0.6867 - acc: 0.5175 - val_loss: 0.7047 - val_acc: 0.4646\n",
      "Epoch 22/25\n",
      " - 4s - loss: 0.6861 - acc: 0.5875 - val_loss: 0.7083 - val_acc: 0.4545\n",
      "Epoch 23/25\n",
      " - 4s - loss: 0.6887 - acc: 0.5450 - val_loss: 0.7125 - val_acc: 0.4747\n",
      "Epoch 24/25\n",
      " - 4s - loss: 0.6864 - acc: 0.5650 - val_loss: 0.7124 - val_acc: 0.4646\n",
      "Epoch 25/25\n",
      " - 4s - loss: 0.6844 - acc: 0.5350 - val_loss: 0.7111 - val_acc: 0.4545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb46770588>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train....\")\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Glove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df.loc[:train_size, 'review'].values\n",
    "train_y = df.loc[:train_size, 'sentiment'].values\n",
    "\n",
    "test_x = df.loc[train_size:total_len, 'review'].values\n",
    "test_y = df.loc[train_size:total_len, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([ len(s.split()) for s in reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148\n"
     ]
    }
   ],
   "source": [
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {}\n",
    "glove_file = 'glove.6B.100d.txt'\n",
    "glove = open(glove_file)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIM = 100\n",
    "emb_matrix = np.zeros((max_length, GLOVE_DIM))\n",
    "\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
