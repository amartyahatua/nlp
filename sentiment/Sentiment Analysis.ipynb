{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df['sentiment']\n",
    "reviews = df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = len(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size= 399\n",
      "Test size= 100\n"
     ]
    }
   ],
   "source": [
    "train_size = int(total_len*0.8)\n",
    "test_size = total_len-train_size\n",
    "\n",
    "print('Train size=',train_size)\n",
    "print('Test size=',test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df.loc[:train_size, 'review'].values\n",
    "train_y = df.loc[:train_size, 'sentiment'].values\n",
    "\n",
    "test_x = df.loc[train_size:total_len, 'review'].values\n",
    "test_y = df.loc[train_size:total_len, 'sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review'].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([ len(s.split()) for s in reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148\n"
     ]
    }
   ],
   "source": [
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n",
    "X_train_tokens = tokenizer.texts_to_sequences(train_x)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1148, 100)         1351500   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,364,301\n",
      "Trainable params: 1,364,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/25\n",
      " - 64s - loss: 0.6952 - acc: 0.5075 - val_loss: 0.6911 - val_acc: 0.5400\n",
      "Epoch 2/25\n",
      " - 6s - loss: 0.6932 - acc: 0.5125 - val_loss: 0.6926 - val_acc: 0.5400\n",
      "Epoch 3/25\n",
      " - 5s - loss: 0.6932 - acc: 0.4925 - val_loss: 0.6930 - val_acc: 0.5400\n",
      "Epoch 4/25\n",
      " - 5s - loss: 0.6927 - acc: 0.5375 - val_loss: 0.6922 - val_acc: 0.5400\n",
      "Epoch 5/25\n",
      " - 5s - loss: 0.6933 - acc: 0.4950 - val_loss: 0.6914 - val_acc: 0.5400\n",
      "Epoch 6/25\n",
      " - 5s - loss: 0.6932 - acc: 0.5100 - val_loss: 0.6908 - val_acc: 0.5400\n",
      "Epoch 7/25\n",
      " - 5s - loss: 0.6934 - acc: 0.5100 - val_loss: 0.6907 - val_acc: 0.5400\n",
      "Epoch 8/25\n",
      " - 5s - loss: 0.6925 - acc: 0.5100 - val_loss: 0.6908 - val_acc: 0.5400\n",
      "Epoch 9/25\n",
      " - 5s - loss: 0.6936 - acc: 0.5100 - val_loss: 0.6911 - val_acc: 0.5400\n",
      "Epoch 10/25\n",
      " - 5s - loss: 0.6937 - acc: 0.5100 - val_loss: 0.6916 - val_acc: 0.5400\n",
      "Epoch 11/25\n",
      " - 5s - loss: 0.6933 - acc: 0.5100 - val_loss: 0.6922 - val_acc: 0.5400\n",
      "Epoch 12/25\n",
      " - 5s - loss: 0.6932 - acc: 0.5175 - val_loss: 0.6923 - val_acc: 0.5400\n",
      "Epoch 13/25\n",
      " - 5s - loss: 0.6923 - acc: 0.5225 - val_loss: 0.6923 - val_acc: 0.5400\n",
      "Epoch 14/25\n",
      " - 5s - loss: 0.6928 - acc: 0.5050 - val_loss: 0.6926 - val_acc: 0.5400\n",
      "Epoch 15/25\n",
      " - 5s - loss: 0.6943 - acc: 0.4225 - val_loss: 0.6932 - val_acc: 0.4700\n",
      "Epoch 16/25\n",
      " - 5s - loss: 0.6933 - acc: 0.4975 - val_loss: 0.6933 - val_acc: 0.4700\n",
      "Epoch 17/25\n",
      " - 6s - loss: 0.6931 - acc: 0.5175 - val_loss: 0.6938 - val_acc: 0.4700\n",
      "Epoch 18/25\n",
      " - 5s - loss: 0.6939 - acc: 0.4975 - val_loss: 0.6946 - val_acc: 0.4600\n",
      "Epoch 19/25\n",
      " - 5s - loss: 0.6930 - acc: 0.4875 - val_loss: 0.6945 - val_acc: 0.4600\n",
      "Epoch 20/25\n",
      " - 5s - loss: 0.6935 - acc: 0.4900 - val_loss: 0.6940 - val_acc: 0.4700\n",
      "Epoch 21/25\n",
      " - 5s - loss: 0.6934 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5400\n",
      "Epoch 22/25\n",
      " - 5s - loss: 0.6931 - acc: 0.5250 - val_loss: 0.6932 - val_acc: 0.4700\n",
      "Epoch 23/25\n",
      " - 5s - loss: 0.6932 - acc: 0.4925 - val_loss: 0.6928 - val_acc: 0.5400\n",
      "Epoch 24/25\n",
      " - 5s - loss: 0.6932 - acc: 0.5125 - val_loss: 0.6919 - val_acc: 0.5400\n",
      "Epoch 25/25\n",
      " - 5s - loss: 0.6921 - acc: 0.5125 - val_loss: 0.6916 - val_acc: 0.5400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcba19d7f28>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train')\n",
    "model.fit(X_train_pad, train_y, epochs = 25, batch_size=128, validation_data=(X_test_pad, test_y), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lines = list()\n",
    "lines = df['review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "for line in lines: \n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped  if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)\n",
    "    \n",
    "    \n",
    "print(len(review_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size =  13203\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=1, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "\n",
    "print(\"Vocab size = \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 0.9999512434005737),\n",
       " ('film', 0.9999445080757141),\n",
       " ('nt', 0.9999390840530396),\n",
       " ('great', 0.9999366402626038),\n",
       " ('well', 0.999934196472168),\n",
       " ('like', 0.9999324679374695),\n",
       " ('see', 0.9999309778213501),\n",
       " ('also', 0.9999294281005859),\n",
       " ('people', 0.9999277591705322),\n",
       " ('even', 0.9999247789382935)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "filename = 'imdb_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "embedding_index = {}\n",
    "f = open(os.path.join('', 'imdb_embedding_word2vec.txt'), encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embedding_index[word] = coefs\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens 13203\n",
      "Shape of review tensor (499, 1148)\n",
      "Shape of sentiment tensor (499,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(review_lines)\n",
    "sequences = tokenizer.texts_to_sequences(review_lines)\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Number of unique tokens\", len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment = df['sentiment'].values\n",
    "\n",
    "print(\"Shape of review tensor\", review_pad.shape)\n",
    "print(\"Shape of sentiment tensor\", sentiment.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word , i in word_index.items():\n",
    "    if(i>num_words):\n",
    "        continue \n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13204\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1148, 100)         1320400   \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,333,201\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 1,320,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_DIM, \n",
    "                            embeddings_initializer=Constant(embedding_matrix), \n",
    "                            input_length = max_length, \n",
    "                            trainable = False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT*review_pad.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train....\n",
      "Train on 400 samples, validate on 99 samples\n",
      "Epoch 1/25\n",
      " - 7s - loss: 0.6933 - acc: 0.5050 - val_loss: 0.6874 - val_acc: 0.5758\n",
      "Epoch 2/25\n",
      " - 4s - loss: 0.6965 - acc: 0.4825 - val_loss: 0.6897 - val_acc: 0.5758\n",
      "Epoch 3/25\n",
      " - 4s - loss: 0.6942 - acc: 0.5000 - val_loss: 0.6996 - val_acc: 0.4444\n",
      "Epoch 4/25\n",
      " - 4s - loss: 0.6962 - acc: 0.5025 - val_loss: 0.7027 - val_acc: 0.4444\n",
      "Epoch 5/25\n",
      " - 4s - loss: 0.6933 - acc: 0.5100 - val_loss: 0.6952 - val_acc: 0.4848\n",
      "Epoch 6/25\n",
      " - 4s - loss: 0.6940 - acc: 0.5075 - val_loss: 0.6894 - val_acc: 0.5758\n",
      "Epoch 7/25\n",
      " - 4s - loss: 0.6947 - acc: 0.5000 - val_loss: 0.6901 - val_acc: 0.5657\n",
      "Epoch 8/25\n",
      " - 4s - loss: 0.6920 - acc: 0.5025 - val_loss: 0.6922 - val_acc: 0.5051\n",
      "Epoch 9/25\n",
      " - 4s - loss: 0.6913 - acc: 0.5200 - val_loss: 0.6918 - val_acc: 0.5152\n",
      "Epoch 10/25\n",
      " - 5s - loss: 0.6945 - acc: 0.5000 - val_loss: 0.6926 - val_acc: 0.5051\n",
      "Epoch 11/25\n",
      " - 5s - loss: 0.6905 - acc: 0.5100 - val_loss: 0.6889 - val_acc: 0.5960\n",
      "Epoch 12/25\n",
      " - 4s - loss: 0.6911 - acc: 0.5300 - val_loss: 0.6847 - val_acc: 0.5859\n",
      "Epoch 13/25\n",
      " - 4s - loss: 0.6902 - acc: 0.5400 - val_loss: 0.6816 - val_acc: 0.5556\n",
      "Epoch 14/25\n",
      " - 4s - loss: 0.6935 - acc: 0.5325 - val_loss: 0.6800 - val_acc: 0.5354\n",
      "Epoch 15/25\n",
      " - 4s - loss: 0.6952 - acc: 0.5350 - val_loss: 0.6802 - val_acc: 0.5556\n",
      "Epoch 16/25\n",
      " - 4s - loss: 0.6941 - acc: 0.5375 - val_loss: 0.6830 - val_acc: 0.5859\n",
      "Epoch 17/25\n",
      " - 4s - loss: 0.6938 - acc: 0.5550 - val_loss: 0.6851 - val_acc: 0.5859\n",
      "Epoch 18/25\n",
      " - 4s - loss: 0.6881 - acc: 0.5750 - val_loss: 0.6856 - val_acc: 0.5758\n",
      "Epoch 19/25\n",
      " - 4s - loss: 0.6914 - acc: 0.5450 - val_loss: 0.6879 - val_acc: 0.5556\n",
      "Epoch 20/25\n",
      " - 4s - loss: 0.6928 - acc: 0.5150 - val_loss: 0.6923 - val_acc: 0.5253\n",
      "Epoch 21/25\n",
      " - 4s - loss: 0.6913 - acc: 0.5375 - val_loss: 0.6908 - val_acc: 0.5556\n",
      "Epoch 22/25\n",
      " - 4s - loss: 0.6882 - acc: 0.5275 - val_loss: 0.6865 - val_acc: 0.5758\n",
      "Epoch 23/25\n",
      " - 4s - loss: 0.6894 - acc: 0.5675 - val_loss: 0.6866 - val_acc: 0.5556\n",
      "Epoch 24/25\n",
      " - 4s - loss: 0.6891 - acc: 0.5550 - val_loss: 0.6857 - val_acc: 0.5657\n",
      "Epoch 25/25\n",
      " - 4s - loss: 0.6853 - acc: 0.5900 - val_loss: 0.6844 - val_acc: 0.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb7387b9b0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train....\")\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
