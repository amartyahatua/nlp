{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_trans_gan.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amartyahatua/nlp/blob/master/test_trans_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8TloR3jTve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import numpy as np \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1XBDLkNELOW",
        "colab_type": "code",
        "outputId": "a249860d-bf2e-45a7-fa44-d511e008591f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0yIXTrQFYKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXMDL7arlNZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "    def lengthWordToIndex(self):\n",
        "        return len(self.word2index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGfrK69ZlRoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijn9LRjClUer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/drive/My Drive/translate/eng_gar.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emcr_586lXay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTbPJ-JclaKm",
        "colab_type": "code",
        "outputId": "10543a84-e7d6-4687-df7a-896186c43a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'gar', True)\n",
        "\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 169813 sentence pairs\n",
            "Trimmed to 9404 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "gar 4434\n",
            "eng 2872\n",
            "['sie sind diejenigen die gehen wollen .', 'they are the ones who want to go .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTZGtAjleww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93-pgjqElzDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=10):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnuLZm1al8nM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RAvdiFrmRzC",
        "colab_type": "code",
        "outputId": "cda5e8ce-9d56-4b52-f9cc-af2d88ba7bce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "encoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/encoder.dict'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkKfekl9mZr6",
        "colab_type": "code",
        "outputId": "73bca51b-4a62-41da-b4d5-c3dc6fc1b41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "attn_decoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/decoder.dict'))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX7qzeOAmnSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(block_dim, block_dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(block_dim, block_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + x\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            *[Block(block_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pVGmTBZn9OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testGan(encoder, decoder, sentence, max_length=10):\n",
        "    with torch.no_grad():\n",
        "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        # print(\"input tensor\",input_tensor.shape)\n",
        "        # print(input_tensor)\n",
        "        # input_length = input_tensor.size()[0]\n",
        "        # print(\"input tensor\",input_length)\n",
        "        # encoder_hidden = encoder.initHidden()\n",
        "        # print(\"encoder_hidden\",encoder_hidden.shape)\n",
        "        # encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "        # print(\"encoder_outputs\",encoder_outputs.shape)\n",
        "\n",
        "        # for ei in range(input_length):\n",
        "        #     encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        #     #print(encoder_output[0, 0].shape)\n",
        "        #     encoder_outputs[ei] += encoder_output[0, 0]\n",
        "        #     print(encoder_outputs)\n",
        "\n",
        "          \n",
        "\n",
        "        latent_dim = 256\n",
        "        n_layers = 20\n",
        "        block_dim = 256\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "        input_length = sentence.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "        #encoder_outputs[0: input_length,:] = sentence\n",
        "        generator = Generator(n_layers, block_dim)\n",
        "        for ei in range(input_length):\n",
        "            noise = torch.from_numpy(np.random.randint(-1, 1, (10, 256))).float()\n",
        "            z = generator(noise)\n",
        "            #encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            #print(encoder_output[0, 0].shape)\n",
        "            encoder_outputs[ei] += z[0, 0]\n",
        "            #print(encoder_outputs)\n",
        "\n",
        "\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "        # print(\"decoder_input\",decoder_input.shape)\n",
        "        # print(\"decoder_hidden\",decoder_hidden.shape)\n",
        "        # print(\"encoder_outputs\",encoder_outputs.shape)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if ((topi.item() == EOS_token) or (topi.item() == '.')):\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbybWM_anoJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import randint\n",
        "\n",
        "def evaluateGAN(encoder, decoder, n=200):\n",
        "    latent_dim = 256\n",
        "    n_layers = 20\n",
        "    block_dim = 256\n",
        "\n",
        "    for i in range(n):\n",
        "        generator = Generator(n_layers, block_dim)\n",
        "        generator.eval()\n",
        "        generator.load_state_dict(torch.load('/content/drive/My Drive/translate/generator.th', map_location='cpu'))\n",
        "        input_tensor = [[85],[86],[109],[1362],[2],[6],[1]]\n",
        "        input_tensor = np.asarray(input_tensor)\n",
        "        noise = torch.from_numpy(np.random.randint(1, 4434, (10, latent_dim)))\n",
        "        #noise = torch.from_numpy(input_tensor).float()\n",
        "        #z = generator(noise)\n",
        "        #print(\"Z shape=\",z.shape)\n",
        "        #print(\"z=\", z)\n",
        "        pair = random.choice(pairs)\n",
        "        #print('>', pair[0])\n",
        "        #print('=', pair[1])\n",
        "        output_words, attentions = testGan(encoder, decoder, noise)\n",
        "        #output_words, attentions = testGan(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        #print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bqo9iSbn8V9",
        "colab_type": "code",
        "outputId": "888d9cb6-42ec-4aaa-dd8f-b5e486110690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluateGAN(encoder1,attn_decoder1)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< i . <EOS>\n",
            "< i m about to do . <EOS>\n",
            "< you re making me me . . . <EOS>\n",
            "< you re making up to the . . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< you re kidding to be in . <EOS>\n",
            "< we . . . <EOS>\n",
            "< you re up to . <EOS>\n",
            "< you . <EOS>\n",
            "< i m thinking . <EOS>\n",
            "< i . . . . . . . . .\n",
            "< you re making a student . . <EOS>\n",
            "< we . . . . . . . . .\n",
            "< you re making the whole . . . <EOS>\n",
            "< you re up to be here . <EOS>\n",
            "< you re making my of me . <EOS>\n",
            "< i . <EOS>\n",
            "< you re a my . <EOS>\n",
            "< you are free . <EOS>\n",
            "< you re making a . <EOS>\n",
            "< i m sorry . <EOS>\n",
            "< i m thinking . . . <EOS>\n",
            "< i m good at the situation . <EOS>\n",
            "< you re up a . <EOS>\n",
            "< i m about to do it . <EOS>\n",
            "< i . . . . . . . . .\n",
            "< i . . . . . . . . .\n",
            "< you are free . <EOS>\n",
            "< i m getting it . <EOS>\n",
            "< you re a student . <EOS>\n",
            "< you re making a . . . . <EOS>\n",
            "< you . <EOS>\n",
            "< you re making the same age . <EOS>\n",
            "< you re making a . <EOS>\n",
            "< . <EOS>\n",
            "< you re in the same age . <EOS>\n",
            "< you re under at the . . <EOS>\n",
            "< you re up to be good . <EOS>\n",
            "< i m coming at the news . <EOS>\n",
            "< i m in the point . <EOS>\n",
            "< you re making me . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< you re making my way . <EOS>\n",
            "< we . . . . . . . . .\n",
            "< you re kidding . . <EOS>\n",
            "< we . . . <EOS>\n",
            "< we . . . . . . . . .\n",
            "< we . . . . . . . . .\n",
            "< you re kidding . <EOS>\n",
            "< i . <EOS>\n",
            "< you are free a . <EOS>\n",
            "< you re making me . <EOS>\n",
            "< you re a high . . . . . .\n",
            "< you re in a arrest . <EOS>\n",
            "< you re making a arrest . <EOS>\n",
            "< i m in a hurry to you . <EOS>\n",
            "< you re making up to me . . . <EOS>\n",
            "< i m about to be here . <EOS>\n",
            "< you re making me . <EOS>\n",
            "< you re making me . <EOS>\n",
            "< you re in a . <EOS>\n",
            "< we re losing . . <EOS>\n",
            "< i . . . . . . . . .\n",
            "< you re a high . <EOS>\n",
            "< you re making me . <EOS>\n",
            "< you ? . <EOS>\n",
            "< you re up to . . <EOS>\n",
            "< you . . . . <EOS>\n",
            "< we . . . <EOS>\n",
            "< you re making me . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< i aren t . . . <EOS>\n",
            "< you re making me . <EOS>\n",
            "< you re making a . <EOS>\n",
            "< i m sorry . <EOS>\n",
            "< i re up to . . . <EOS>\n",
            "< i . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< i m up to . . . . <EOS>\n",
            "< you . . . . . . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< i . . . . . . . . .\n",
            "< you re making a . . . . . <EOS>\n",
            "< you . <EOS>\n",
            "< i re up . . . . . . .\n",
            "< i m in the same age . <EOS>\n",
            "< you re making a arrest . . <EOS>\n",
            "< you re making my way . <EOS>\n",
            "< i m in the point . . <EOS>\n",
            "< i m sorry to think . . . <EOS>\n",
            "< i m sorry . <EOS>\n",
            "< you re making a arrest . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< i m sorry . <EOS>\n",
            "< i re going to be . . <EOS>\n",
            "< i re up . . . . . . .\n",
            "< we re up . <EOS>\n",
            "< i m sorry . . <EOS>\n",
            "< we . . . . . . . . .\n",
            "< . <EOS>\n",
            "< you . . . . . . . <EOS>\n",
            "< we re up . . . . . . .\n",
            "< you re up to the me . <EOS>\n",
            "< i m about to . . . <EOS>\n",
            "< you re playing right . <EOS>\n",
            "< we . . . . . . . . .\n",
            "< i m sorry . <EOS>\n",
            "< we . . . . . . . . .\n",
            "< you re in the same age . <EOS>\n",
            "< i re up . <EOS>\n",
            "< we . . <EOS>\n",
            "< i m having the same . <EOS>\n",
            "< we . . <EOS>\n",
            "< i m getting old and . . . <EOS>\n",
            "< i m in the same age . <EOS>\n",
            "< i m sorry . <EOS>\n",
            "< i re going to the . . . <EOS>\n",
            "< you re a good and . . . <EOS>\n",
            "< you re up to me . <EOS>\n",
            "< you are free a . <EOS>\n",
            "< you re kidding to the . . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< you re playing a . . . <EOS>\n",
            "< i re up to the . . <EOS>\n",
            "< you re making me . <EOS>\n",
            "< i re up to . . . <EOS>\n",
            "< you re a about the . . <EOS>\n",
            "< you re making a walk . . . <EOS>\n",
            "< you re up . <EOS>\n",
            "< you re up to be . . <EOS>\n",
            "< you re making a . . . . . .\n",
            "< you re kidding to the . . <EOS>\n",
            "< i re getting . . <EOS>\n",
            "< we . <EOS>\n",
            "< you re up to be here . . . <EOS>\n",
            "< you are . <EOS>\n",
            "< you re up to be . . <EOS>\n",
            "< you re kidding . . . . . <EOS>\n",
            "< i m sorry about the . . . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< you ? <EOS>\n",
            "< you are free . <EOS>\n",
            "< you re a good we . . <EOS>\n",
            "< you are free a . <EOS>\n",
            "< i m sorry . . . . . . <EOS>\n",
            "< i m sorry . <EOS>\n",
            "< i m sorry . <EOS>\n",
            "< we . . . . . . . . .\n",
            "< you re making a . . . . . .\n",
            "< you re making me . <EOS>\n",
            "< i . . . . . . . . .\n",
            "< i re doing . . . . . . .\n",
            "< i m sorry . <EOS>\n",
            "< we re up to . . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< i . . . . . . <EOS>\n",
            "< you re joking my own . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< you re making a friend . <EOS>\n",
            "< i . . . . . . . . .\n",
            "< i m sorry for you . . . <EOS>\n",
            "< i m sorry . <EOS>\n",
            "< . . . . <EOS>\n",
            "< i m getting it . <EOS>\n",
            "< we re up . <EOS>\n",
            "< you re under at my way . <EOS>\n",
            "< i m up to the . . <EOS>\n",
            "< you re making a arrest . <EOS>\n",
            "< i m in the point . . . <EOS>\n",
            "< you re making the whole . . . . .\n",
            "< i . . <EOS>\n",
            "< we re up . . . . . . .\n",
            "< you . <EOS>\n",
            "< we . . <EOS>\n",
            "< you re kidding . . . . . . .\n",
            "< you re up to me . <EOS>\n",
            "< i m sorry . . . <EOS>\n",
            "< you re a to my own . <EOS>\n",
            "< you re up to the . . . . .\n",
            "< you re making a boy . <EOS>\n",
            "< i m sorry . <EOS>\n",
            "< i m sorry to the . . . . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< you are free . <EOS>\n",
            "< you re making a walk . <EOS>\n",
            "< i . . . . . . . . .\n",
            "< you re kidding . <EOS>\n",
            "< you is good at the moment . . <EOS>\n",
            "< you re a to be . <EOS>\n",
            "< i . . . . . <EOS>\n",
            "< you are free . <EOS>\n",
            "< you re kidding . <EOS>\n",
            "< i m up to . <EOS>\n",
            "< we re . . . . . . . .\n",
            "< i re kidding . <EOS>\n",
            "< you re making a my . <EOS>\n",
            "< you re making my way . <EOS>\n",
            "< you re making a friend . <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}