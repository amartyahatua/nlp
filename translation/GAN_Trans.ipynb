{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_Trans.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONnh5Gjj5G1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import numpy as np \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3QxVy016iih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RErv0Hrv7zwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMIBIUDz6qWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('%s_%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AvBfP0y60Yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTdx5GxG69GV",
        "colab_type": "code",
        "outputId": "cb358131-2d1a-4cd9-ca73-40a5c6fcff1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'gar', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 169813 sentence pairs\n",
            "Trimmed to 9404 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "gar 4434\n",
            "eng 2872\n",
            "['er ist hier nicht mehr willkommen .', 'he is no longer welcome here .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riVnJUIS-oZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq9bjoQ--YWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW2Y2JZb_Cnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBAdTWGB_FtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 256\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxvQ3f08_lo-",
        "colab_type": "code",
        "outputId": "3ce83eb9-12e6-4c43-e1fa-625f617f1c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoder1.load_state_dict(torch.load('encoder.dict'))\n",
        "attn_decoder1.load_state_dict(torch.load('decoder.dict'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4-bIy1s_8sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_layers = 20\n",
        "block_dim = 256\n",
        "gp_lambda = 10\n",
        "latent_dim = 256\n",
        "interval = 1000\n",
        "batch_size = 1\n",
        "n_critic = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FmGNFptAD3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(block_dim, block_dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(block_dim, block_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + x\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            *[Block(block_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            *[Block(block_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2R-0oQLCP8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Generator(n_layers, block_dim)\n",
        "critic = Critic(n_layers, block_dim)\n",
        "critic.to(device)\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\n",
        "c_optimizer = optim.Adam(critic.parameters(), lr=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1OTGvmcFirp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_grad_penalty(critic, real_data, fake_data):\n",
        "    B = real_data.size(0)\n",
        "    alpha = torch.FloatTensor(np.random.random((B, 1)))\n",
        "    alpha = alpha.to(device)\n",
        "    sample = alpha*real_data + (1-alpha)*fake_data\n",
        "    sample.requires_grad_(True)\n",
        "    sample = sample.to(device)\n",
        "    score = critic(sample)\n",
        "    outputs = torch.FloatTensor(B, 256).fill_(1.0)\n",
        "    outputs.requires_grad_(False)\n",
        "    outputs = outputs.to(device)\n",
        " \n",
        "    grads = autograd.grad(\n",
        "        outputs=score,\n",
        "        inputs=sample,\n",
        "        grad_outputs=outputs,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "    #grads = grads.view(B, -1)\n",
        "    grad_penalty = ((grads.norm(2, dim=1) - 1.) ** 2).mean()\n",
        "\n",
        "    return grad_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNA9ib_FDvgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "\n",
        "\n",
        "    generator = Generator(n_layers, block_dim)\n",
        "    generator.to(device)\n",
        "    critic = Critic(n_layers, block_dim)\n",
        "    critic.to(device)\n",
        "\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\n",
        "    c_optimizer = optim.Adam(critic.parameters(), lr=1e-4)\n",
        "    \n",
        "    encoder1.eval()\n",
        "    attn_decoder1.eval()\n",
        "    generator.train()\n",
        "    critic.train()\n",
        "\n",
        "    c_train_loss = 0.\n",
        "    g_train_loss = 0.\n",
        "    g_batches = 0\n",
        "    hidden_size = 256\n",
        "    max_length = 10\n",
        "\n",
        "    for i in range(len(pairs)):\n",
        "      pair = pairs[i]\n",
        "      sentence = pair[0]\n",
        "\n",
        "      input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "      input_length = input_tensor.size()[0]\n",
        "\n",
        "      encoder_hidden = encoder1.initHidden()\n",
        "      encoder_outputs = torch.zeros(input_length, encoder1.hidden_size, device=device)\n",
        "        \n",
        "      decoder_output = torch.zeros(max_length, latent_dim, device=device)\n",
        "\n",
        "      fc3 = nn.Linear(encoder1.hidden_size, latent_dim)\n",
        "      fc3.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for ei in range(input_length):\n",
        "              encoder_output, encoder_hidden = encoder1(input_tensor[ei], encoder_hidden)\n",
        "              encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "      encoder_outputs = fc3(encoder_outputs)\n",
        "      c_optimizer.zero_grad()\n",
        "\n",
        "      noise = torch.from_numpy(np.random.normal(0, 1, (input_length, latent_dim))).float()\n",
        "      noise = noise.to(device)\n",
        "\n",
        "      z_fake = generator(noise)        \n",
        "      z_fake.to(device)\n",
        "\n",
        "      real_score = critic(encoder_outputs)\n",
        "      fake_score = critic(z_fake)\n",
        "      grad_penalty = compute_grad_penalty(critic, encoder_outputs.data, z_fake.data)\n",
        "      \n",
        "      c_loss = -torch.mean(real_score) + torch.mean(fake_score) + gp_lambda * grad_penalty\n",
        "      c_train_loss += c_loss.item()\n",
        "      c_loss.backward()\n",
        "      c_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      # train generator\n",
        "      if i % n_critic == 0:\n",
        "          g_batches += 1\n",
        "          g_optimizer.zero_grad()\n",
        "          fake_score = critic(generator(noise))\n",
        "          g_loss = -torch.mean(fake_score)\n",
        "          g_train_loss += g_loss.item()\n",
        "          g_loss.backward()\n",
        "          g_optimizer.step()\n",
        "\n",
        "      if interval > 0 and i % interval == 0:\n",
        "          print('Epoch: {} | Batch: {}/{} ({:.0f}%) | G Loss: {:.6f} | C Loss: {:.6f}'.format(\n",
        "              epoch, batch_size * i, len(pairs),\n",
        "                      100. * (batch_size * i) / len(pairs),\n",
        "              g_loss.item(), c_loss.item()\n",
        "          ))\n",
        "\n",
        "    print(\"End of loop ====>>>>>\")\n",
        "    g_train_loss /= g_batches\n",
        "    c_train_loss /= len(pairs)\n",
        "    print('* (Train) Epoch: {} | G Loss: {:.4f} | C Loss: {:.4f}'.format(\n",
        "        epoch, g_train_loss, c_train_loss\n",
        "    ))\n",
        "    return (g_train_loss, c_train_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tflZce9SJnrJ",
        "colab_type": "code",
        "outputId": "a9e484b2-6af5-4c7e-8ec3-2eea8265d64a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_loss = np.inf\n",
        "epochs = 10\n",
        "for epoch in range(1, epochs + 1):\n",
        "    g_loss, c_loss = train(epoch)\n",
        "    loss = g_loss + c_loss\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        print('* Saved')\n",
        "        torch.save(generator.state_dict(), 'generator.th')\n",
        "        torch.save(critic.state_dict(), 'critic.th')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Batch: 0/9404 (0%) | G Loss: 0.086658 | C Loss: 6440.877930\n",
            "Epoch: 1 | Batch: 1000/9404 (11%) | G Loss: -0.117293 | C Loss: 0.301929\n",
            "Epoch: 1 | Batch: 2000/9404 (21%) | G Loss: -0.109819 | C Loss: 1.829215\n",
            "Epoch: 1 | Batch: 3000/9404 (32%) | G Loss: -0.115603 | C Loss: 0.374288\n",
            "Epoch: 1 | Batch: 4000/9404 (43%) | G Loss: -0.100903 | C Loss: 0.092565\n",
            "Epoch: 1 | Batch: 5000/9404 (53%) | G Loss: -0.062586 | C Loss: 0.030611\n",
            "Epoch: 1 | Batch: 6000/9404 (64%) | G Loss: -0.014667 | C Loss: 0.314916\n",
            "Epoch: 1 | Batch: 7000/9404 (74%) | G Loss: -0.036270 | C Loss: 0.051012\n",
            "Epoch: 1 | Batch: 8000/9404 (85%) | G Loss: -0.018941 | C Loss: 0.003692\n",
            "Epoch: 1 | Batch: 9000/9404 (96%) | G Loss: 0.002555 | C Loss: 0.041357\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 1 | G Loss: -0.0556 | C Loss: 6.3920\n",
            "* Saved\n",
            "Epoch: 2 | Batch: 0/9404 (0%) | G Loss: 0.253180 | C Loss: 8267.447266\n",
            "Epoch: 2 | Batch: 1000/9404 (11%) | G Loss: -0.004784 | C Loss: 0.029291\n",
            "Epoch: 2 | Batch: 2000/9404 (21%) | G Loss: -0.081482 | C Loss: 0.347111\n",
            "Epoch: 2 | Batch: 3000/9404 (32%) | G Loss: -0.110176 | C Loss: 0.125372\n",
            "Epoch: 2 | Batch: 4000/9404 (43%) | G Loss: -0.059523 | C Loss: 0.338136\n",
            "Epoch: 2 | Batch: 5000/9404 (53%) | G Loss: 0.003275 | C Loss: 0.098760\n",
            "Epoch: 2 | Batch: 6000/9404 (64%) | G Loss: -0.031520 | C Loss: 0.203362\n",
            "Epoch: 2 | Batch: 7000/9404 (74%) | G Loss: -0.002099 | C Loss: 0.022292\n",
            "Epoch: 2 | Batch: 8000/9404 (85%) | G Loss: 0.022430 | C Loss: -0.009364\n",
            "Epoch: 2 | Batch: 9000/9404 (96%) | G Loss: 0.018842 | C Loss: -0.016829\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 2 | G Loss: -0.0258 | C Loss: 7.5125\n",
            "Epoch: 3 | Batch: 0/9404 (0%) | G Loss: 0.063463 | C Loss: 7260.778320\n",
            "Epoch: 3 | Batch: 1000/9404 (11%) | G Loss: -0.004148 | C Loss: -0.026826\n",
            "Epoch: 3 | Batch: 2000/9404 (21%) | G Loss: -0.068346 | C Loss: 0.402375\n",
            "Epoch: 3 | Batch: 3000/9404 (32%) | G Loss: -0.098436 | C Loss: 0.200083\n",
            "Epoch: 3 | Batch: 4000/9404 (43%) | G Loss: -0.038666 | C Loss: 0.044388\n",
            "Epoch: 3 | Batch: 5000/9404 (53%) | G Loss: -0.005362 | C Loss: 0.177010\n",
            "Epoch: 3 | Batch: 6000/9404 (64%) | G Loss: -0.001235 | C Loss: 0.047469\n",
            "Epoch: 3 | Batch: 7000/9404 (74%) | G Loss: 0.011438 | C Loss: 0.002555\n",
            "Epoch: 3 | Batch: 8000/9404 (85%) | G Loss: 0.030177 | C Loss: 0.001586\n",
            "Epoch: 3 | Batch: 9000/9404 (96%) | G Loss: 0.039965 | C Loss: 0.006829\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 3 | G Loss: -0.0172 | C Loss: 7.0361\n",
            "Epoch: 4 | Batch: 0/9404 (0%) | G Loss: -0.025746 | C Loss: 5559.176758\n",
            "Epoch: 4 | Batch: 1000/9404 (11%) | G Loss: -0.170105 | C Loss: 0.773534\n",
            "Epoch: 4 | Batch: 2000/9404 (21%) | G Loss: -0.160307 | C Loss: 0.181847\n",
            "Epoch: 4 | Batch: 3000/9404 (32%) | G Loss: -0.175271 | C Loss: 0.096332\n",
            "Epoch: 4 | Batch: 4000/9404 (43%) | G Loss: -0.120743 | C Loss: 0.432090\n",
            "Epoch: 4 | Batch: 5000/9404 (53%) | G Loss: -0.121062 | C Loss: 0.025645\n",
            "Epoch: 4 | Batch: 6000/9404 (64%) | G Loss: -0.072192 | C Loss: 0.029403\n",
            "Epoch: 4 | Batch: 7000/9404 (74%) | G Loss: -0.058770 | C Loss: 0.008374\n",
            "Epoch: 4 | Batch: 8000/9404 (85%) | G Loss: -0.037855 | C Loss: 0.026187\n",
            "Epoch: 4 | Batch: 9000/9404 (96%) | G Loss: -0.041008 | C Loss: 0.014946\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 4 | G Loss: -0.0915 | C Loss: 6.1235\n",
            "* Saved\n",
            "Epoch: 5 | Batch: 0/9404 (0%) | G Loss: 0.210739 | C Loss: 7658.506348\n",
            "Epoch: 5 | Batch: 1000/9404 (11%) | G Loss: -0.143983 | C Loss: 0.181024\n",
            "Epoch: 5 | Batch: 2000/9404 (21%) | G Loss: -0.140575 | C Loss: 0.183139\n",
            "Epoch: 5 | Batch: 3000/9404 (32%) | G Loss: -0.139846 | C Loss: 0.885414\n",
            "Epoch: 5 | Batch: 4000/9404 (43%) | G Loss: -0.076159 | C Loss: 0.063823\n",
            "Epoch: 5 | Batch: 5000/9404 (53%) | G Loss: -0.015602 | C Loss: 0.033989\n",
            "Epoch: 5 | Batch: 6000/9404 (64%) | G Loss: -0.029834 | C Loss: 0.038391\n",
            "Epoch: 5 | Batch: 7000/9404 (74%) | G Loss: -0.050203 | C Loss: 0.032473\n",
            "Epoch: 5 | Batch: 8000/9404 (85%) | G Loss: -0.015009 | C Loss: 0.007801\n",
            "Epoch: 5 | Batch: 9000/9404 (96%) | G Loss: -0.001715 | C Loss: 0.015491\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 5 | G Loss: -0.0533 | C Loss: 7.2885\n",
            "Epoch: 6 | Batch: 0/9404 (0%) | G Loss: -0.107948 | C Loss: 6567.877441\n",
            "Epoch: 6 | Batch: 1000/9404 (11%) | G Loss: 0.000556 | C Loss: 0.171358\n",
            "Epoch: 6 | Batch: 2000/9404 (21%) | G Loss: -0.110159 | C Loss: 0.200888\n",
            "Epoch: 6 | Batch: 3000/9404 (32%) | G Loss: -0.140052 | C Loss: 0.036065\n",
            "Epoch: 6 | Batch: 4000/9404 (43%) | G Loss: -0.212947 | C Loss: 0.311511\n",
            "Epoch: 6 | Batch: 5000/9404 (53%) | G Loss: -0.193791 | C Loss: 0.037806\n",
            "Epoch: 6 | Batch: 6000/9404 (64%) | G Loss: -0.146442 | C Loss: -0.010222\n",
            "Epoch: 6 | Batch: 7000/9404 (74%) | G Loss: -0.117937 | C Loss: 0.005610\n",
            "Epoch: 6 | Batch: 8000/9404 (85%) | G Loss: -0.090918 | C Loss: 0.011242\n",
            "Epoch: 6 | Batch: 9000/9404 (96%) | G Loss: -0.062306 | C Loss: -0.004572\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 6 | G Loss: -0.1131 | C Loss: 6.8659\n",
            "Epoch: 7 | Batch: 0/9404 (0%) | G Loss: -0.068213 | C Loss: 6493.604492\n",
            "Epoch: 7 | Batch: 1000/9404 (11%) | G Loss: -0.147255 | C Loss: 0.119452\n",
            "Epoch: 7 | Batch: 2000/9404 (21%) | G Loss: -0.081423 | C Loss: -0.014347\n",
            "Epoch: 7 | Batch: 3000/9404 (32%) | G Loss: -0.090442 | C Loss: 0.340771\n",
            "Epoch: 7 | Batch: 4000/9404 (43%) | G Loss: -0.029373 | C Loss: 0.134916\n",
            "Epoch: 7 | Batch: 5000/9404 (53%) | G Loss: 0.022912 | C Loss: -0.014170\n",
            "Epoch: 7 | Batch: 6000/9404 (64%) | G Loss: -0.024283 | C Loss: 0.051082\n",
            "Epoch: 7 | Batch: 7000/9404 (74%) | G Loss: -0.002275 | C Loss: 0.027865\n",
            "Epoch: 7 | Batch: 8000/9404 (85%) | G Loss: 0.025082 | C Loss: -0.000729\n",
            "Epoch: 7 | Batch: 9000/9404 (96%) | G Loss: 0.035009 | C Loss: 0.003765\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 7 | G Loss: -0.0032 | C Loss: 6.7650\n",
            "Epoch: 8 | Batch: 0/9404 (0%) | G Loss: 0.082390 | C Loss: 6165.141602\n",
            "Epoch: 8 | Batch: 1000/9404 (11%) | G Loss: -0.095072 | C Loss: 3.356154\n",
            "Epoch: 8 | Batch: 2000/9404 (21%) | G Loss: -0.147603 | C Loss: 1.412496\n",
            "Epoch: 8 | Batch: 3000/9404 (32%) | G Loss: -0.071325 | C Loss: 0.151647\n",
            "Epoch: 8 | Batch: 4000/9404 (43%) | G Loss: -0.025983 | C Loss: 0.197392\n",
            "Epoch: 8 | Batch: 5000/9404 (53%) | G Loss: -0.067907 | C Loss: 0.224337\n",
            "Epoch: 8 | Batch: 6000/9404 (64%) | G Loss: -0.081739 | C Loss: 0.048314\n",
            "Epoch: 8 | Batch: 7000/9404 (74%) | G Loss: -0.055770 | C Loss: 0.007114\n",
            "Epoch: 8 | Batch: 8000/9404 (85%) | G Loss: -0.045076 | C Loss: 0.029945\n",
            "Epoch: 8 | Batch: 9000/9404 (96%) | G Loss: -0.043482 | C Loss: 0.005324\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 8 | G Loss: -0.0573 | C Loss: 6.7583\n",
            "Epoch: 9 | Batch: 0/9404 (0%) | G Loss: 0.067154 | C Loss: 7184.184082\n",
            "Epoch: 9 | Batch: 1000/9404 (11%) | G Loss: 0.017353 | C Loss: 2.657638\n",
            "Epoch: 9 | Batch: 2000/9404 (21%) | G Loss: -0.098989 | C Loss: 2.811129\n",
            "Epoch: 9 | Batch: 3000/9404 (32%) | G Loss: -0.096293 | C Loss: 0.757221\n",
            "Epoch: 9 | Batch: 4000/9404 (43%) | G Loss: -0.017724 | C Loss: 0.230092\n",
            "Epoch: 9 | Batch: 5000/9404 (53%) | G Loss: -0.006712 | C Loss: 0.076276\n",
            "Epoch: 9 | Batch: 6000/9404 (64%) | G Loss: -0.008589 | C Loss: 0.029436\n",
            "Epoch: 9 | Batch: 7000/9404 (74%) | G Loss: 0.026357 | C Loss: 0.013508\n",
            "Epoch: 9 | Batch: 8000/9404 (85%) | G Loss: 0.043277 | C Loss: 0.007857\n",
            "Epoch: 9 | Batch: 9000/9404 (96%) | G Loss: 0.046041 | C Loss: 0.026263\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 9 | G Loss: -0.0010 | C Loss: 6.4798\n",
            "Epoch: 10 | Batch: 0/9404 (0%) | G Loss: -0.051108 | C Loss: 7874.391602\n",
            "Epoch: 10 | Batch: 1000/9404 (11%) | G Loss: 0.008335 | C Loss: 0.182948\n",
            "Epoch: 10 | Batch: 2000/9404 (21%) | G Loss: -0.116775 | C Loss: 0.622332\n",
            "Epoch: 10 | Batch: 3000/9404 (32%) | G Loss: -0.090872 | C Loss: 0.167202\n",
            "Epoch: 10 | Batch: 4000/9404 (43%) | G Loss: -0.054690 | C Loss: -0.023691\n",
            "Epoch: 10 | Batch: 5000/9404 (53%) | G Loss: -0.044161 | C Loss: 0.027589\n",
            "Epoch: 10 | Batch: 6000/9404 (64%) | G Loss: -0.028611 | C Loss: 0.103418\n",
            "Epoch: 10 | Batch: 7000/9404 (74%) | G Loss: -0.012274 | C Loss: -0.009161\n",
            "Epoch: 10 | Batch: 8000/9404 (85%) | G Loss: 0.008241 | C Loss: -0.003640\n",
            "Epoch: 10 | Batch: 9000/9404 (96%) | G Loss: 0.045655 | C Loss: -0.016101\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 10 | G Loss: -0.0334 | C Loss: 7.0619\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}