{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tran_en_de_new_qus_ans.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeNlMa1us3-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFRE6obUtIIG",
        "colab_type": "code",
        "outputId": "9181a25a-3324-4f85-b43f-adaa80825415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(device)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl4hoqTpcOYV",
        "colab_type": "code",
        "outputId": "162c735d-29dd-44c2-b41b-5710c3dd672f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l3k7I9vtc2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfMDYMIutf8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SScdLVc4tjLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filterLong(pairs):\n",
        "    pairList = []\n",
        "    MAX_LENGTH = 10\n",
        "    for i in range(len(pairs)):\n",
        "        if(len(pairs[i])>1):\n",
        "          sent0 = pairs[i][0]\n",
        "          sent1 = pairs[i][1]\n",
        "          \n",
        "        if(\" \" in sent0):\n",
        "            temoSent0 = sent0.split(\" \")\n",
        "        if(\" \" in sent1):\n",
        "            temoSent1 = sent1.split(\" \")\n",
        "\n",
        "        if((len(temoSent0) <= MAX_LENGTH) and (len(temoSent1) <= MAX_LENGTH)):\n",
        "          pairList.append(pairs[i])\n",
        "    return pairList\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/drive/My Drive/translate/question_answer.txt').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdwEiDz7tl-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkq6zAWduJl0",
        "colab_type": "code",
        "outputId": "98cd1b1c-4e44-4ac9-ca5c-3f409c2ca859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterLong(pairs)\n",
        "    twoConv = []\n",
        "    for i in range(len(pairs)):\n",
        "        if(len(pairs[i]) >1):\n",
        "            twoConv.append(pairs[i])\n",
        "\n",
        "\n",
        "    pairs = filterPairs(twoConv)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        if(len(pair) > 1):\n",
        "          input_lang.addSentence(pair[0])\n",
        "          output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'gar', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 699281 sentence pairs\n",
            "Trimmed to 12879 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "gar 80\n",
            "eng 81\n",
            "['thanks .', 'you re welcome .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iz7-vNguSBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        #print(\"coming here\")\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHxDCRVXv364",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        #embedded = self.dropout(embedded)\n",
        "        #print(\"Embd size-->\",embedded.size)\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay3ek-IiwBPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0SywPOmwFYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf3lKxMiwI40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## With attention\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # if use_teacher_forcing:\n",
        "    #     # Teacher forcing: Feed the target as the next input\n",
        "    #     for di in range(target_length):\n",
        "    #         decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "    #             decoder_input, decoder_hidden, encoder_outputs)\n",
        "    #         loss += criterion(decoder_output, target_tensor[di])\n",
        "    #         decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    # else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "    for di in range(target_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "        if decoder_input.item() == EOS_token:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBuaCiwdwPtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_zEwCdUwPys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=10, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "    #return plot_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3P108aAwP3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1], encoder_outputs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QVNzeLwaOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomlyTranslate(encoder, decoder, n=100):\n",
        "    encoder_outputs = []\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions, en_out = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "        #print(en_out)\n",
        "        # maxVal = max(en_out)\n",
        "        # minVal = min(en_out)\n",
        "\n",
        "        # print(\"Max value->\",maxVal)\n",
        "        # print(\"Min value->\",minVal)   \n",
        "\n",
        "        encoder_outputs.append(en_out)\n",
        "\n",
        "    return encoder_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phom_ONA1s8G",
        "colab_type": "code",
        "outputId": "eabec08b-251d-4557-8a5c-a4e1f3111261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "encoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/encoder_tran_en_de_new_qus_ans.dict'))\n",
        "attn_decoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/decoder_with_attention_tran_en_de_new_qus_ans.dict'))\n",
        "\n",
        "encoder_outputs = evaluateRandomlyTranslate(encoder1, attn_decoder1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> eight .\n",
            "= they re all so cute .\n",
            "< they re all so cute . <EOS>\n",
            "\n",
            "> what are they doing ?\n",
            "= they are admiring the beauty of the birds .\n",
            "< they are admiring the beauty of the birds . <EOS>\n",
            "\n",
            "> what are they doing ?\n",
            "= they are admiring the beauty of the birds .\n",
            "< they are admiring the beauty of the birds . <EOS>\n",
            "\n",
            "> how has mass media changed recently ?\n",
            "= they re more modern and user friendly .\n",
            "< they re more modern and user friendly . <EOS>\n",
            "\n",
            "> thanks .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> no i always back up my files .\n",
            "= you re smart .\n",
            "< you re smart . <EOS>\n",
            "\n",
            "> what are they doing ?\n",
            "= they are admiring the beauty of the birds .\n",
            "< they are admiring the beauty of the birds . <EOS>\n",
            "\n",
            "> it s supposed to be unlucky .\n",
            "= you re supposed to stay home all day .\n",
            "< you re supposed to stay home all day . <EOS>\n",
            "\n",
            "> my husband died .\n",
            "= i m sorry for you .\n",
            "< i m sorry for you . <EOS>\n",
            "\n",
            "> i mean you re wasting your life .\n",
            "= i m having fun .\n",
            "< i m having fun . <EOS>\n",
            "\n",
            "> where is your brother studying ?\n",
            "= he is studying in us .\n",
            "< he is studying in us . <EOS>\n",
            "\n",
            "> no i always back up my files .\n",
            "= you re smart .\n",
            "< you re smart . <EOS>\n",
            "\n",
            "> why are you yawning ?\n",
            "= i m sleepy .\n",
            "< i m sleepy . <EOS>\n",
            "\n",
            "> why are you so late ?\n",
            "= i m sorry but i missed the bus .\n",
            "< i m sorry but i missed the bus . <EOS>\n",
            "\n",
            "> where are you studying now ?\n",
            "= i am studying in government engineering college .\n",
            "< i am studying in government engineering college . <EOS>\n",
            "\n",
            "> thank you so much .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> thanks .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> eight .\n",
            "= they re all so cute .\n",
            "< they re all so cute . <EOS>\n",
            "\n",
            "> it s supposed to be unlucky .\n",
            "= you re supposed to stay home all day .\n",
            "< you re supposed to stay home all day . <EOS>\n",
            "\n",
            "> why are you yawning ?\n",
            "= i m sleepy .\n",
            "< i m sleepy . <EOS>\n",
            "\n",
            "> where is your brother studying ?\n",
            "= he is studying in us .\n",
            "< he is studying in us . <EOS>\n",
            "\n",
            "> how have you been ?\n",
            "= i am fine thanks !\n",
            "< i am fine thanks ! <EOS>\n",
            "\n",
            "> where are you studying now ?\n",
            "= i am studying in government engineering college .\n",
            "< i am studying in government engineering college . <EOS>\n",
            "\n",
            "> the people are friendly .\n",
            "= i m not ever going to leave .\n",
            "< i m not ever going to leave . <EOS>\n",
            "\n",
            "> what s your job ?\n",
            "= i m a teacher .\n",
            "< i m a teacher . <EOS>\n",
            "\n",
            "> what size are you ?\n",
            "= i m an extra large .\n",
            "< i m an extra large . <EOS>\n",
            "\n",
            "> how are people there ?\n",
            "= they re friendly and hospitable .\n",
            "< they re friendly and hospitable . <EOS>\n",
            "\n",
            "> what are you going to have for breakfast ?\n",
            "= i m thinking about having some eggs .\n",
            "< i m thinking about having some eggs . <EOS>\n",
            "\n",
            "> they will return only after half an hour .\n",
            "= i am following my friends to take bath .\n",
            "< i am following my friends to take bath . <EOS>\n",
            "\n",
            "> i promise i will . thanks a million .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> i mean you re wasting your life .\n",
            "= i m having fun .\n",
            "< i m having fun . <EOS>\n",
            "\n",
            "> why are you so late ?\n",
            "= i m sorry but i missed the bus .\n",
            "< i m sorry but i missed the bus . <EOS>\n",
            "\n",
            "> what kind of music do you like ?\n",
            "= i m crazy about pop music .\n",
            "< i m crazy about pop music . <EOS>\n",
            "\n",
            "> the people are friendly .\n",
            "= i m not ever going to leave .\n",
            "< i m not ever going to leave . <EOS>\n",
            "\n",
            "> hello . how are you ?\n",
            "= i m fine ok well thank you\n",
            "< i m fine ok well thank you <EOS>\n",
            "\n",
            "> what are your measurements ?\n",
            "= i m a waist and a inseam .\n",
            "< i m a waist and a inseam . <EOS>\n",
            "\n",
            "> what are your measurements ?\n",
            "= i m a waist and a inseam .\n",
            "< i m a waist and a inseam . <EOS>\n",
            "\n",
            "> thank you .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> why are you yawning ?\n",
            "= i m sleepy .\n",
            "< i m sleepy . <EOS>\n",
            "\n",
            "> thank you so much .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> what are you going to have for breakfast ?\n",
            "= i m thinking about having some eggs .\n",
            "< i m thinking about having some eggs . <EOS>\n",
            "\n",
            "> i mean you re wasting your life .\n",
            "= i m having fun .\n",
            "< i m having fun . <EOS>\n",
            "\n",
            "> eight .\n",
            "= they re all so cute .\n",
            "< they re all so cute . <EOS>\n",
            "\n",
            "> no i always back up my files .\n",
            "= you re smart .\n",
            "< you re smart . <EOS>\n",
            "\n",
            "> what are you going to have for breakfast ?\n",
            "= i m thinking about having some eggs .\n",
            "< i m thinking about having some eggs . <EOS>\n",
            "\n",
            "> what is appealing about her ?\n",
            "= she s a democratic politician .\n",
            "< she s a democratic politician . <EOS>\n",
            "\n",
            "> what are your measurements ?\n",
            "= i m a waist and a inseam .\n",
            "< i m a waist and a inseam . <EOS>\n",
            "\n",
            "> what kind of music do you like ?\n",
            "= i m crazy about pop music .\n",
            "< i m crazy about pop music . <EOS>\n",
            "\n",
            "> where is your brother studying ?\n",
            "= he is studying in us .\n",
            "< he is studying in us . <EOS>\n",
            "\n",
            "> thank you .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> what s for dinner ?\n",
            "= i m not sure .\n",
            "< i m not sure . <EOS>\n",
            "\n",
            "> my husband died .\n",
            "= i m sorry for you .\n",
            "< i m sorry for you . <EOS>\n",
            "\n",
            "> what kind of music do you like ?\n",
            "= i m crazy about pop music .\n",
            "< i m crazy about pop music . <EOS>\n",
            "\n",
            "> i promise i will . thanks a million .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> no i always back up my files .\n",
            "= you re smart .\n",
            "< you re smart . <EOS>\n",
            "\n",
            "> what size are you ?\n",
            "= i m an extra large .\n",
            "< i m an extra large . <EOS>\n",
            "\n",
            "> hello . how are you ?\n",
            "= i m fine ok well thank you\n",
            "< i m fine ok well thank you <EOS>\n",
            "\n",
            "> eight .\n",
            "= they re all so cute .\n",
            "< they re all so cute . <EOS>\n",
            "\n",
            "> it s supposed to be unlucky .\n",
            "= you re supposed to stay home all day .\n",
            "< you re supposed to stay home all day . <EOS>\n",
            "\n",
            "> thank you so much .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> they will return only after half an hour .\n",
            "= i am following my friends to take bath .\n",
            "< i am following my friends to take bath . <EOS>\n",
            "\n",
            "> how has mass media changed recently ?\n",
            "= they re more modern and user friendly .\n",
            "< they re more modern and user friendly . <EOS>\n",
            "\n",
            "> thank you so much .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> what kind of music do you like ?\n",
            "= i m crazy about pop music .\n",
            "< i m crazy about pop music . <EOS>\n",
            "\n",
            "> what is appealing about her ?\n",
            "= she s a democratic politician .\n",
            "< she s a democratic politician . <EOS>\n",
            "\n",
            "> my husband died .\n",
            "= i m sorry for you .\n",
            "< i m sorry for you . <EOS>\n",
            "\n",
            "> i mean you re wasting your life .\n",
            "= i m having fun .\n",
            "< i m having fun . <EOS>\n",
            "\n",
            "> hello . how are you ?\n",
            "= i m fine ok well thank you\n",
            "< i m fine ok well thank you <EOS>\n",
            "\n",
            "> how are people there ?\n",
            "= they re friendly and hospitable .\n",
            "< they re friendly and hospitable . <EOS>\n",
            "\n",
            "> what s your job ?\n",
            "= i m a teacher .\n",
            "< i m a teacher . <EOS>\n",
            "\n",
            "> thank you .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> no i always back up my files .\n",
            "= you re smart .\n",
            "< you re smart . <EOS>\n",
            "\n",
            "> they will return only after half an hour .\n",
            "= i am following my friends to take bath .\n",
            "< i am following my friends to take bath . <EOS>\n",
            "\n",
            "> what kind of music do you like ?\n",
            "= i m crazy about pop music .\n",
            "< i m crazy about pop music . <EOS>\n",
            "\n",
            "> how are people there ?\n",
            "= they re friendly and hospitable .\n",
            "< they re friendly and hospitable . <EOS>\n",
            "\n",
            "> i mean you re wasting your life .\n",
            "= i m having fun .\n",
            "< i m having fun . <EOS>\n",
            "\n",
            "> the people are friendly .\n",
            "= i m not ever going to leave .\n",
            "< i m not ever going to leave . <EOS>\n",
            "\n",
            "> how are people there ?\n",
            "= they re friendly and hospitable .\n",
            "< they re friendly and hospitable . <EOS>\n",
            "\n",
            "> how has mass media changed recently ?\n",
            "= they re more modern and user friendly .\n",
            "< they re more modern and user friendly . <EOS>\n",
            "\n",
            "> the people are friendly .\n",
            "= i m not ever going to leave .\n",
            "< i m not ever going to leave . <EOS>\n",
            "\n",
            "> why are you so late ?\n",
            "= i m sorry but i missed the bus .\n",
            "< i m sorry but i missed the bus . <EOS>\n",
            "\n",
            "> what are you going to have for breakfast ?\n",
            "= i m thinking about having some eggs .\n",
            "< i m thinking about having some eggs . <EOS>\n",
            "\n",
            "> why are you yawning ?\n",
            "= i m sleepy .\n",
            "< i m sleepy . <EOS>\n",
            "\n",
            "> how have you been ?\n",
            "= i am fine thanks !\n",
            "< i am fine thanks ! <EOS>\n",
            "\n",
            "> thank you so much .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> what are you going to have for breakfast ?\n",
            "= i m thinking about having some eggs .\n",
            "< i m thinking about having some eggs . <EOS>\n",
            "\n",
            "> why are you so late ?\n",
            "= i m sorry but i missed the bus .\n",
            "< i m sorry but i missed the bus . <EOS>\n",
            "\n",
            "> what are they doing ?\n",
            "= they are admiring the beauty of the birds .\n",
            "< they are admiring the beauty of the birds . <EOS>\n",
            "\n",
            "> where are you studying now ?\n",
            "= i am studying in government engineering college .\n",
            "< i am studying in government engineering college . <EOS>\n",
            "\n",
            "> what is appealing about her ?\n",
            "= she s a democratic politician .\n",
            "< she s a democratic politician . <EOS>\n",
            "\n",
            "> how are people there ?\n",
            "= they re friendly and hospitable .\n",
            "< they re friendly and hospitable . <EOS>\n",
            "\n",
            "> why are you so late ?\n",
            "= i m sorry but i missed the bus .\n",
            "< i m sorry but i missed the bus . <EOS>\n",
            "\n",
            "> what are your measurements ?\n",
            "= i m a waist and a inseam .\n",
            "< i m a waist and a inseam . <EOS>\n",
            "\n",
            "> what s for dinner ?\n",
            "= i m not sure .\n",
            "< i m not sure . <EOS>\n",
            "\n",
            "> what size are you ?\n",
            "= i m an extra large .\n",
            "< i m an extra large . <EOS>\n",
            "\n",
            "> what are they doing ?\n",
            "= they are admiring the beauty of the birds .\n",
            "< they are admiring the beauty of the birds . <EOS>\n",
            "\n",
            "> i mean you re wasting your life .\n",
            "= i m having fun .\n",
            "< i m having fun . <EOS>\n",
            "\n",
            "> thanks .\n",
            "= you re welcome .\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "> the people are friendly .\n",
            "= i m not ever going to leave .\n",
            "< i m not ever going to leave . <EOS>\n",
            "\n",
            "> what s your job ?\n",
            "= i m a teacher .\n",
            "< i m a teacher . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vRM2VcVWh83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51bd76e7-5b55-4dc0-9e8d-1d079c4c67ac"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "encoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/encoder_tran_en_de_new_qus_ans.dict'))\n",
        "attn_decoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/decoder_with_attention_tran_en_de_new_qus_ans.dict'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfBpiiqY5Sks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(block_dim, block_dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(block_dim, block_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + x\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            *[Block(block_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            *[Block(block_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxgEIBQC6gTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_layers = 20\n",
        "block_dim = 256\n",
        "gp_lambda = 10\n",
        "latent_dim = 256\n",
        "interval = 1000\n",
        "batch_size = 1\n",
        "n_critic = 100\n",
        "generator = Generator(n_layers, block_dim)\n",
        "critic = Critic(n_layers, block_dim)\n",
        "critic.to(device)\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\n",
        "c_optimizer = optim.Adam(critic.parameters(), lr=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh0v7sAF5fqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testGan(encoder, decoder, sentence, max_length=10):\n",
        "    with torch.no_grad():\n",
        "        #print(\"1\")\n",
        "        input_length = sentence.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "        encoder_outputs[0: input_length,:] = sentence\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "        \n",
        "        #print(\"2\")\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            #print(\"3\")\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOhbuXuo5mnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import randint\n",
        "\n",
        "def evaluateGAN(encoder, decoder, encoder_outputs, n=100):\n",
        "    latent_dim = 256\n",
        "    n_layers = 20\n",
        "    block_dim = 256\n",
        "    output_sentence_list = []\n",
        "    while(len(output_sentence_list) < 100):\n",
        "        generator = Generator(n_layers, block_dim)\n",
        "        generator.eval()\n",
        "        generator.load_state_dict(torch.load('/content/drive/My Drive/translate/generator_question_answer.th', map_location='cpu'))\n",
        "        input_tensor = [[85],[86],[109],[1362],[2],[6],[1]]\n",
        "        input_tensor = torch.from_numpy(np.asarray(input_tensor)).float()\n",
        "        noise = torch.from_numpy(np.random.normal(loc = 0, scale = 1, size = (10,256)))\n",
        "        \n",
        "        #noise = torch.from_numpy(input_tensor).float()\n",
        "        #z = generator(noise)\n",
        "        #print(\"Z shape=\",z.shape)\n",
        "        #print(\"z=\", z)\n",
        "        pair = random.choice(pairs)\n",
        "        #print('>', pair[0])\n",
        "        #print('=', pair[1])\n",
        "        #print(noise)\n",
        "        #output_words, attentions = testGan(encoder, decoder, encoder_outputs[i])\n",
        "        output_words, attentions = testGan(encoder, decoder, noise)\n",
        "        #output_words, attentions = testGan(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "\n",
        "        if(output_sentence not in output_sentence_list):\n",
        "          output_sentence_list.append(output_sentence)\n",
        "          print('<', output_sentence)\n",
        "          print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLSTvU_o5m6A",
        "colab_type": "code",
        "outputId": "833c3c9d-ba83-4912-a9bd-b36712852c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluateGAN(encoder1,attn_decoder1, encoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< i m not sure . <EOS>\n",
            "\n",
            "< you re supposed to stay home all day . <EOS>\n",
            "\n",
            "< you re smart . <EOS>\n",
            "\n",
            "< you re welcome . <EOS>\n",
            "\n",
            "< i m not ever going to leave . <EOS>\n",
            "\n",
            "< i m a waist and user friendly . <EOS>\n",
            "\n",
            "< i m not ever going to take bath . <EOS>\n",
            "\n",
            "< i m fine thanks ! <EOS>\n",
            "\n",
            "< i m having fun . <EOS>\n",
            "\n",
            "< i m not sure and user friendly . <EOS>\n",
            "\n",
            "< i m not sure and hospitable . <EOS>\n",
            "\n",
            "< you re more modern and user friendly . <EOS>\n",
            "\n",
            "< i m not ever going to <EOS>\n",
            "\n",
            "< i m sorry sure . <EOS>\n",
            "\n",
            "< i m a teacher . <EOS>\n",
            "\n",
            "< i m not ever going . <EOS>\n",
            "\n",
            "< i m not sure and user . <EOS>\n",
            "\n",
            "< i m not sure to take bath . <EOS>\n",
            "\n",
            "< i m not sure sure . <EOS>\n",
            "\n",
            "< i m not sure to stay home all day .\n",
            "\n",
            "< i m sorry for you . <EOS>\n",
            "\n",
            "< i am following my friends to take bath . <EOS>\n",
            "\n",
            "< i m a teacher and user friendly . <EOS>\n",
            "\n",
            "< they re all so cute . <EOS>\n",
            "\n",
            "< they re more modern and user friendly . <EOS>\n",
            "\n",
            "< i m not sure to take . <EOS>\n",
            "\n",
            "< i m fine ok well thank you . <EOS>\n",
            "\n",
            "< they re friendly and hospitable . <EOS>\n",
            "\n",
            "< you re supposed . <EOS>\n",
            "\n",
            "< i m not sure and user <EOS>\n",
            "\n",
            "< i m a democratic politician . <EOS>\n",
            "\n",
            "< you re supposed to stay home all . <EOS>\n",
            "\n",
            "< you re all so cute . <EOS>\n",
            "\n",
            "< i m not sure friends to take bath . <EOS>\n",
            "\n",
            "< i m not sure to stay home <EOS>\n",
            "\n",
            "< i m not ever going to take . <EOS>\n",
            "\n",
            "< i m fine ok well thank you <EOS>\n",
            "\n",
            "< i m fine sure . <EOS>\n",
            "\n",
            "< i m not sure to leave . <EOS>\n",
            "\n",
            "< i m a teacher and hospitable . <EOS>\n",
            "\n",
            "< you re supposed to stay home <EOS>\n",
            "\n",
            "< i m crazy about pop music . <EOS>\n",
            "\n",
            "< i m not sure to stay home all . <EOS>\n",
            "\n",
            "< i am fine thanks ! <EOS>\n",
            "\n",
            "< i m a teacher and user <EOS>\n",
            "\n",
            "< you re smart to stay home all day . <EOS>\n",
            "\n",
            "< i m not sure well thank you . <EOS>\n",
            "\n",
            "< they re supposed to stay home all day . <EOS>\n",
            "\n",
            "< i m fine thanks cute . <EOS>\n",
            "\n",
            "< they re friendly and user friendly . <EOS>\n",
            "\n",
            "< i m fine ok well . <EOS>\n",
            "\n",
            "< i m a waist and user <EOS>\n",
            "\n",
            "< i m not sure my friends to take bath .\n",
            "\n",
            "< i m not sure going to <EOS>\n",
            "\n",
            "< you re friendly . <EOS>\n",
            "\n",
            "< i m not sure going to take bath . <EOS>\n",
            "\n",
            "< i m fine sure to take bath . <EOS>\n",
            "\n",
            "< i m not sure and a inseam . <EOS>\n",
            "\n",
            "< i m sorry sure to stay home all . <EOS>\n",
            "\n",
            "< i am following my friends to take . <EOS>\n",
            "\n",
            "< you re more modern and user <EOS>\n",
            "\n",
            "< i m a waist and hospitable . <EOS>\n",
            "\n",
            "< i m sorry for . <EOS>\n",
            "\n",
            "< i m a waist and a <EOS>\n",
            "\n",
            "< i m a waist and <EOS>\n",
            "\n",
            "< they re more modern and user . <EOS>\n",
            "\n",
            "< i m a waist and a inseam . <EOS>\n",
            "\n",
            "< you re more modern . <EOS>\n",
            "\n",
            "< i m fine ok well thank . <EOS>\n",
            "\n",
            "< i m sorry sure to take bath . <EOS>\n",
            "\n",
            "< i m sorry for and user friendly . <EOS>\n",
            "\n",
            "< they re more modern and user <EOS>\n",
            "\n",
            "< i m a waist and a . <EOS>\n",
            "\n",
            "< i m sorry ever going to take bath . <EOS>\n",
            "\n",
            "< i m sorry sure and user friendly . <EOS>\n",
            "\n",
            "< i m not sure you . <EOS>\n",
            "\n",
            "< you re all day . <EOS>\n",
            "\n",
            "< i m not sure well thank you <EOS>\n",
            "\n",
            "< i m sorry ever going to <EOS>\n",
            "\n",
            "< i m a waist and . <EOS>\n",
            "\n",
            "< i m a waist and user . <EOS>\n",
            "\n",
            "< you re supposed to stay home all <EOS>\n",
            "\n",
            "< i m crazy about having . <EOS>\n",
            "\n",
            "< i m sorry . <EOS>\n",
            "\n",
            "< i m a teacher and user . <EOS>\n",
            "\n",
            "< i m a teacher you . <EOS>\n",
            "\n",
            "< i m a teacher and <EOS>\n",
            "\n",
            "< i m sorry sure to take . <EOS>\n",
            "\n",
            "< i m not sure cute . <EOS>\n",
            "\n",
            "< i m not sure to stay home all <EOS>\n",
            "\n",
            "< i m not sure and a <EOS>\n",
            "\n",
            "< you re smart to take bath . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX9QqyHM5mvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnYnq-eB5mtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}