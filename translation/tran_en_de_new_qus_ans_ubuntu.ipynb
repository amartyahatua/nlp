{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tran_en_de_new_qus_ans_ubuntu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeNlMa1us3-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFRE6obUtIIG",
        "colab_type": "code",
        "outputId": "83ea3a82-b535-4773-d00f-348d8136b480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(device)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl4hoqTpcOYV",
        "colab_type": "code",
        "outputId": "f4d4359f-3d60-46fc-8af7-66104cd3e40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l3k7I9vtc2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfMDYMIutf8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SScdLVc4tjLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filterLong(pairs):\n",
        "    pairList = []\n",
        "    MAX_LENGTH = 10\n",
        "    for i in range(len(pairs)):\n",
        "        if(len(pairs[i])>1):\n",
        "          sent0 = pairs[i][0]\n",
        "          sent1 = pairs[i][1]\n",
        "          \n",
        "        if(\" \" in sent0):\n",
        "            temoSent0 = sent0.split(\" \")\n",
        "        if(\" \" in sent1):\n",
        "            temoSent1 = sent1.split(\" \")\n",
        "\n",
        "        if((len(temoSent0) <= MAX_LENGTH) and (len(temoSent1) <= MAX_LENGTH)):\n",
        "          pairList.append(pairs[i])\n",
        "    return pairList\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/drive/My Drive/translate/questions_answers_ubuntu.txt').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdwEiDz7tl-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkq6zAWduJl0",
        "colab_type": "code",
        "outputId": "02433f8f-2f64-44fe-81e6-d3aff9349eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterLong(pairs)\n",
        "    twoConv = []\n",
        "    for i in range(len(pairs)):\n",
        "        if(len(pairs[i]) >1):\n",
        "            twoConv.append(pairs[i])\n",
        "\n",
        "\n",
        "    pairs = filterPairs(twoConv)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        if(len(pair) > 1):\n",
        "          input_lang.addSentence(pair[0])\n",
        "          output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'gar', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 969572 sentence pairs\n",
            "Trimmed to 2738 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "gar 1528\n",
            "eng 1413\n",
            "['nto yet having reached my best before date', 'i m still immature i m sure']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iz7-vNguSBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        #print(\"coming here\")\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHxDCRVXv364",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        #embedded = self.dropout(embedded)\n",
        "        #print(\"Embd size-->\",embedded.size)\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay3ek-IiwBPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0SywPOmwFYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf3lKxMiwI40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## With attention\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # if use_teacher_forcing:\n",
        "    #     # Teacher forcing: Feed the target as the next input\n",
        "    #     for di in range(target_length):\n",
        "    #         decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "    #             decoder_input, decoder_hidden, encoder_outputs)\n",
        "    #         loss += criterion(decoder_output, target_tensor[di])\n",
        "    #         decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    # else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "    for di in range(target_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "        if decoder_input.item() == EOS_token:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBuaCiwdwPtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_zEwCdUwPys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=10, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "    #return plot_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3P108aAwP3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1], encoder_outputs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QVNzeLwaOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomlyTranslate(encoder, decoder, n=100):\n",
        "    encoder_outputs = []\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions, en_out = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "        #print(en_out)\n",
        "        # maxVal = max(en_out)\n",
        "        # minVal = min(en_out)\n",
        "\n",
        "        # print(\"Max value->\",maxVal)\n",
        "        # print(\"Min value->\",minVal)   \n",
        "\n",
        "        encoder_outputs.append(en_out)\n",
        "\n",
        "    return encoder_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phom_ONA1s8G",
        "colab_type": "code",
        "outputId": "92d0cce5-211a-4266-889f-358401b836f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        }
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 1s (- 14m 27s) (5000 6%) 2.4107\n",
            "2m 2s (- 13m 19s) (10000 13%) 2.1481\n",
            "3m 9s (- 12m 37s) (15000 20%) 1.8025\n",
            "4m 19s (- 11m 54s) (20000 26%) 1.3309\n",
            "5m 33s (- 11m 6s) (25000 33%) 0.8546\n",
            "6m 49s (- 10m 13s) (30000 40%) 0.5281\n",
            "8m 6s (- 9m 16s) (35000 46%) 0.3220\n",
            "9m 24s (- 8m 14s) (40000 53%) 0.2138\n",
            "10m 42s (- 7m 8s) (45000 60%) 0.1731\n",
            "12m 0s (- 6m 0s) (50000 66%) 0.1574\n",
            "13m 19s (- 4m 50s) (55000 73%) 0.1604\n",
            "14m 37s (- 3m 39s) (60000 80%) 0.1458\n",
            "15m 56s (- 2m 27s) (65000 86%) 0.1459\n",
            "17m 15s (- 1m 13s) (70000 93%) 0.1346\n",
            "18m 34s (- 0m 0s) (75000 100%) 0.1217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wop_BVi8Wlov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfe00b65-8198-4213-d7b3-8f630687f069"
      },
      "source": [
        "\n",
        "torch.save(encoder1.state_dict(), '/content/drive/My Drive/translate/encoder_tran_en_de_new_qus_ans_ubuntu.dict.th')\n",
        "torch.save(attn_decoder1.state_dict(), '/content/drive/My Drive/translate/decoder_with_attention_tran_en_de_new_qus_ans_ubuntu.dict')\n",
        "encoder_outputs = evaluateRandomlyTranslate(encoder1, attn_decoder1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> isn t his router ip . ?\n",
            "= i am aware\n",
            "< i am aware <EOS>\n",
            "\n",
            "> i am brazilian\n",
            "= i am pt\n",
            "< i am pt <EOS>\n",
            "\n",
            "> why ?\n",
            "= you re going to regret that\n",
            "< i re looking streets regret flash <EOS>\n",
            "\n",
            "> ok think i gonna try them\n",
            "= they are really good\n",
            "< they are really good <EOS>\n",
            "\n",
            "> hullo\n",
            "= i m off now as havn t instaled\n",
            "< i m off now as havn t instaled <EOS>\n",
            "\n",
            "> thx so i propably got broken iso img\n",
            "= i m running it now\n",
            "< i m running it now <EOS>\n",
            "\n",
            "> i have no idea about what to download\n",
            "= i m sure it is available\n",
            "< i m sure it is available <EOS>\n",
            "\n",
            "> hello\n",
            "= we are\n",
            "< we am using in irc <EOS>\n",
            "\n",
            "> your like . . . ?\n",
            "= you are gay like anal sex with other men\n",
            "< you are gay like anal sex with other men <EOS>\n",
            "\n",
            "> any error messages ?\n",
            "= i m a linux newbie . . .\n",
            "< i m a linux newbie . . . <EOS>\n",
            "\n",
            "> why is that ?\n",
            "= i am not very pleased with ubuntu\n",
            "< i am not very pleased with ubuntu <EOS>\n",
            "\n",
            ">  !iso obi \n",
            "= i m just not sure which options to use\n",
            "< i m just not sure which options to use <EOS>\n",
            "\n",
            "> what does dmesg tail say ?\n",
            "= i m having crashy hangy problems\n",
            "< i m having crashy hangy problems <EOS>\n",
            "\n",
            "> boots too \n",
            "= i m gonna die \n",
            "< i m gonna die  <EOS>\n",
            "\n",
            "> how about hipo ipod management tool\n",
            "= i m tring to find it one sex utarpradesh\n",
            "< i m tring to find it one sex utarpradesh <EOS>\n",
            "\n",
            "> no problem . everybody was new at some time\n",
            "= you are looking for kernel\n",
            "< you are looking for kernel <EOS>\n",
            "\n",
            "> anyone knows an open source irc channel ?\n",
            "= i m need a lil help anyone free ?\n",
            "< i m need a lil help anyone free ? <EOS>\n",
            "\n",
            "> yes\n",
            "= i m in the english chan for ubuntu ?\n",
            "< you m having qemu id <EOS>\n",
            "\n",
            ">  etc network interfaces\n",
            "= i m use to etc sysconfig network scripts\n",
            "< i m use to etc sysconfig network scripts <EOS>\n",
            "\n",
            "> can anyone help ?\n",
            "= i m having troubles with my wireless\n",
            "< i m having troubles with my wireless <EOS>\n",
            "\n",
            "> fluxbox\n",
            "= i m getting sick of gnome s bloatware .\n",
            "< i m getting sick of gnome s bloatware . <EOS>\n",
            "\n",
            "> please stop .\n",
            "= i am not the channel operator\n",
            "< i am not <EOS>\n",
            "\n",
            "> and i just want to say it s great\n",
            "= i m a newish ubuntu user\n",
            "< i m a newish ubuntu user <EOS>\n",
            "\n",
            "> really ?\n",
            "= i m happy now\n",
            "< i m so confused <EOS>\n",
            "\n",
            "> what is this gnome tool called ?\n",
            "= i m using kubuntu \n",
            "< i m using kubuntu  <EOS>\n",
            "\n",
            ">  dmesg no ?\n",
            "= i am trying to debug some login issues\n",
            "< i am trying to debug some login issues <EOS>\n",
            "\n",
            "> invalid operation dvtm\n",
            "= i m running ubuntu server\n",
            "< i m running ubuntu server <EOS>\n",
            "\n",
            "> no idea !\n",
            "= i m a trained sound eng\n",
            "< i m a trained sound eng <EOS>\n",
            "\n",
            "> mount the windows partitions\n",
            "= i m sort of new to linux\n",
            "< i m sort of new to linux <EOS>\n",
            "\n",
            "> is virtualbox cool ?\n",
            "= i m installing win with virtualbox\n",
            "< i m installing win with virtualbox <EOS>\n",
            "\n",
            "> you can resize\n",
            "= they are relatively simple\n",
            "< they are relatively simple <EOS>\n",
            "\n",
            "> what s the problem ?\n",
            "= i m new to linux\n",
            "< i m having to linux <EOS>\n",
            "\n",
            "> boots too \n",
            "= i m gonna die \n",
            "< i m gonna die  <EOS>\n",
            "\n",
            "> well there s vgrename i think\n",
            "= i m trying to rename few vg lv\n",
            "< i m trying to rename few vg lv <EOS>\n",
            "\n",
            "> yeah right\n",
            "= we re living on a globe remember ?\n",
            "< we re living on a globe remember ? <EOS>\n",
            "\n",
            "> os open source . . sorry for the confusion\n",
            "= they are better than the os ones at least\n",
            "< they are better than the os ones at least <EOS>\n",
            "\n",
            "> no i m not brazilian\n",
            "= you are brazilian ?\n",
            "< you are brazilian ? <EOS>\n",
            "\n",
            ">  \n",
            "= i am not a software package \n",
            "< you re here  <EOS>\n",
            "\n",
            "> read https wiki .ubuntu .com restrictedformats\n",
            "= i am using flash player i believe\n",
            "< i am using flash player i believe <EOS>\n",
            "\n",
            "> aticonfig initial should set up a basic one\n",
            "= i m using ati drivers\n",
            "< i m using ati drivers <EOS>\n",
            "\n",
            "> yeah i m on squeeze atm\n",
            "= i m on debian lenny but have plugdev too\n",
            "< i m on debian lenny but have plugdev too <EOS>\n",
            "\n",
            "> why ?\n",
            "= you re going to regret that\n",
            "< i re looking streets regret flash <EOS>\n",
            "\n",
            "> you re a very good bot\n",
            "= i m just talking quickly !\n",
            "< i m just talking quickly ! <EOS>\n",
            "\n",
            "> be more specific plz\n",
            "= i m using onboard\n",
            "< i m using onboard <EOS>\n",
            "\n",
            "> there you go help the man\n",
            "= i m not sure about gnome though\n",
            "< i m not sure about gnome though <EOS>\n",
            "\n",
            "> hello welcome aboard\n",
            "= i m new\n",
            "< i m new <EOS>\n",
            "\n",
            "> i have no problems with . . lol\n",
            "= you re not running the latest version of openoffice\n",
            "< you re not running the latest version of openoffice <EOS>\n",
            "\n",
            "> sorry i don t use anti virus .\n",
            "= i m thinking about cpu and ram ussage\n",
            "< i m thinking about cpu and ram ussage <EOS>\n",
            "\n",
            "> don t work as root !\n",
            "= i am newb\n",
            "< i am newb <EOS>\n",
            "\n",
            "> nto yet having reached my best before date\n",
            "= i m still immature i m sure\n",
            "< i m still immature i m sure <EOS>\n",
            "\n",
            "> fluxbox\n",
            "= i m getting sick of gnome s bloatware .\n",
            "< i m getting sick of gnome s bloatware . <EOS>\n",
            "\n",
            "> cd desktop\n",
            "= i m having trouble cd ing to my desktop\n",
            "< i m having trouble cd ing to my desktop <EOS>\n",
            "\n",
            "> that probably wont be too useful .\n",
            "= i m interested in knowing that too . \n",
            "< i m interested in knowing that too .  <EOS>\n",
            "\n",
            "> is the master bot of the hive\n",
            "= we are all bots\n",
            "< we are all bots <EOS>\n",
            "\n",
            "> ok sorry\n",
            "= i m on . gnome . \n",
            "< i m on . gnome .  <EOS>\n",
            "\n",
            "> choose xubuntu or some other lighter derivative .\n",
            "= i m so frustrated ! !\n",
            "< i m so frustrated ! ! <EOS>\n",
            "\n",
            "> nice \n",
            "= i am impressed with ubuntu .\n",
            "< i am impressed with ubuntu . <EOS>\n",
            "\n",
            "> it s in \n",
            "= i m trying to find my .purple folder\n",
            "< i m trying to find my .purple folder <EOS>\n",
            "\n",
            "> debian questions are offtopic for this channel .\n",
            "= they are all spleeping ! ! ! !\n",
            "< they are all spleeping ! ! ! ! <EOS>\n",
            "\n",
            "> opensuse\n",
            "= i am instlaling suse\n",
            "< i am instlaling suse <EOS>\n",
            "\n",
            "> not any more he s not\n",
            "= he s also doing the same in offtopic\n",
            "< he s also doing the same in offtopic <EOS>\n",
            "\n",
            "> possibly ask in ubuntu server\n",
            "= they are all gigabit lines also\n",
            "< they are all gigabit lines also <EOS>\n",
            "\n",
            "> classy .\n",
            "= i m not a newfag like you\n",
            "< i m not a newfag like you <EOS>\n",
            "\n",
            "> no i m not using vnc\n",
            "= you aren t connecting with vnc are you ?\n",
            "< you aren t connecting with vnc are you ? <EOS>\n",
            "\n",
            "> actually very good\n",
            "= i m on razor qt\n",
            "< i m on razor qt <EOS>\n",
            "\n",
            "> can any body help me ? in private plz\n",
            "= i am a newcomer and installed my first ubuntu\n",
            "< i am a newcomer and installed my first ubuntu <EOS>\n",
            "\n",
            "> in terminal\n",
            "= i am looking for a manual for beryl\n",
            "< i am looking for a manual for beryl <EOS>\n",
            "\n",
            "> maybe pulse is the only supported option now ?\n",
            "= i m pretty sure it does\n",
            "< i m pretty sure it does <EOS>\n",
            "\n",
            "> yeah right\n",
            "= we re living on a globe remember ?\n",
            "< we re living on a globe remember ? <EOS>\n",
            "\n",
            "> does anyone know the fedora support channel ?\n",
            "= i am ditching ubuntu for the times beings\n",
            "< i am ditching ubuntu for the times beings <EOS>\n",
            "\n",
            "> debian\n",
            "= you re not making sense . . .\n",
            "< you re not making sense . . . <EOS>\n",
            "\n",
            "> yeah it s great isn t it ?\n",
            "= i m gay .\n",
            "< i m gay . <EOS>\n",
            "\n",
            ">  home isn t anyone s home directory .\n",
            "= i m sure thats in the starter guide .\n",
            "< i m sure thats in the starter guide . <EOS>\n",
            "\n",
            "> probally just something with the web page\n",
            "= i am sorry\n",
            "< i am sorry <EOS>\n",
            "\n",
            "> it s not better it s different try it\n",
            "= they are different os s\n",
            "< they are different os s <EOS>\n",
            "\n",
            "> from dos . to hardy heron \n",
            "= i am multi booting here \n",
            "< i am multi booting here  <EOS>\n",
            "\n",
            ">  waves goodbye to bill gates \n",
            "= i m a new ubuntu user basically it rocks\n",
            "< i m a new ubuntu user basically it rocks <EOS>\n",
            "\n",
            "> yap i think its awant\n",
            "= i m just not getting good luck with that\n",
            "< i m just not getting good luck with that <EOS>\n",
            "\n",
            "> yeah i dnt think it will affect that\n",
            "= i m about to find out p\n",
            "< i m about to find out p <EOS>\n",
            "\n",
            ">  waves goodbye to bill gates \n",
            "= i m a new ubuntu user basically it rocks\n",
            "< i m a new ubuntu user basically it rocks <EOS>\n",
            "\n",
            "> i want to see how they install it\n",
            "= i am looking for a woman to install ubuntu\n",
            "< i am looking for a woman to install ubuntu <EOS>\n",
            "\n",
            "> well there s vgrename i think\n",
            "= i m trying to rename few vg lv\n",
            "< i m trying to rename few vg lv <EOS>\n",
            "\n",
            "> you still need to see the hardy room\n",
            "= i m still running gutsy though .\n",
            "< i m still running gutsy though . <EOS>\n",
            "\n",
            "> hehe\n",
            "= he s gone\n",
            "< he s gone <EOS>\n",
            "\n",
            "> can anyone tell how to install opera\n",
            "= i m having problems\n",
            "< i m having problems <EOS>\n",
            "\n",
            "> still no luck with samba \n",
            "= you re back\n",
            "< you re back <EOS>\n",
            "\n",
            "> wanna try beryl ?\n",
            "= i am having some beryl trouble\n",
            "< i am having some beryl trouble <EOS>\n",
            "\n",
            "> update manager is the gui for apt get upgrade\n",
            "= they re the same thing\n",
            "< they re the same thing <EOS>\n",
            "\n",
            "> well im not laughing\n",
            "= i m encountering a funny problem\n",
            "< i m encountering a funny problem <EOS>\n",
            "\n",
            "> me too \n",
            "= i m more of a geek in training .\n",
            "< i am new over a <EOS>\n",
            "\n",
            "> try msg nickserv help\n",
            "= i am mka now\n",
            "< i am mka now <EOS>\n",
            "\n",
            "> command more\n",
            "= i am confused about pipelining\n",
            "< i am confused about pipelining <EOS>\n",
            "\n",
            "> so is most of the channel\n",
            "= i m a newbie\n",
            "< i m a newbie <EOS>\n",
            "\n",
            "> https edge .launchpad .net ubuntu ppas\n",
            "= i m confused as to how they work .\n",
            "< i m confused as to how they work . <EOS>\n",
            "\n",
            "> welcome to ubuntu\n",
            "= i m new to ubuntu\n",
            "< i m on ubuntu good <EOS>\n",
            "\n",
            "> great . . how about you ?\n",
            "= i am also trying to intall mono\n",
            "< i am also trying to intall mono <EOS>\n",
            "\n",
            "> and short\n",
            "= i m ssh x ing into my ubuntu box\n",
            "< i m ssh x ing into my ubuntu box <EOS>\n",
            "\n",
            "> hehe\n",
            "= you aren t using ntfs \n",
            "< he s gone <EOS>\n",
            "\n",
            "> it works right out of box .\n",
            "= i am using linksys wusb n\n",
            "< i am using linksys wusb n <EOS>\n",
            "\n",
            "> ah . . . thanks\n",
            "= i m not on gnome usually including now \n",
            "< i m not on gnome usually including now  <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vRM2VcVWh83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d867c622-1b35-4483-9545-6268240e2725"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "encoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/encoder_tran_en_de_new_qus_ans_ubuntu.dict.th'))\n",
        "attn_decoder1.load_state_dict(torch.load('/content/drive/My Drive/translate/decoder_with_attention_tran_en_de_new_qus_ans_ubuntu.dict'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfBpiiqY5Sks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(block_dim, block_dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(block_dim, block_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + x\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            *[Block(block_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, block_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            *[Block(block_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxgEIBQC6gTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_layers = 20\n",
        "block_dim = 256\n",
        "gp_lambda = 10\n",
        "latent_dim = 256\n",
        "interval = 1000\n",
        "batch_size = 1\n",
        "n_critic = 100\n",
        "generator = Generator(n_layers, block_dim)\n",
        "critic = Critic(n_layers, block_dim)\n",
        "critic.to(device)\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\n",
        "c_optimizer = optim.Adam(critic.parameters(), lr=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31QTXXDocVJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_grad_penalty(critic, real_data, fake_data):\n",
        "    B = real_data.size(0)\n",
        "    alpha = torch.FloatTensor(np.random.random((B, 1)))\n",
        "    alpha = alpha.to(device)\n",
        "    sample = alpha*real_data + (1-alpha)*fake_data\n",
        "    sample.requires_grad_(True)\n",
        "    sample = sample.to(device)\n",
        "    score = critic(sample)\n",
        "    outputs = torch.FloatTensor(B, 256).fill_(1.0)\n",
        "    outputs.requires_grad_(False)\n",
        "    outputs = outputs.to(device)\n",
        " \n",
        "    grads = autograd.grad(\n",
        "        outputs=score,\n",
        "        inputs=sample,\n",
        "        grad_outputs=outputs,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "    #grads = grads.view(B, -1)\n",
        "    grad_penalty = ((grads.norm(2, dim=1) - 1.) ** 2).mean()\n",
        "\n",
        "    return grad_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh4joi4RcXC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "\n",
        "\n",
        "    generator = Generator(n_layers, block_dim)\n",
        "    generator.to(device)\n",
        "    critic = Critic(n_layers, block_dim)\n",
        "    critic.to(device)\n",
        "\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\n",
        "    c_optimizer = optim.Adam(critic.parameters(), lr=1e-4)\n",
        "    \n",
        "    encoder1.eval()\n",
        "    attn_decoder1.eval()\n",
        "    generator.train()\n",
        "    critic.train()\n",
        "\n",
        "    c_train_loss = 0.\n",
        "    g_train_loss = 0.\n",
        "    g_batches = 0\n",
        "    hidden_size = 256\n",
        "    max_length = 10\n",
        "\n",
        "    for i in range(len(pairs)):\n",
        "      pair = pairs[i]\n",
        "      sentence = pair[0]\n",
        "\n",
        "      input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "      input_length = input_tensor.size()[0]\n",
        "\n",
        "      encoder_hidden = encoder1.initHidden()\n",
        "      encoder_outputs = torch.zeros(input_length, encoder1.hidden_size, device=device)\n",
        "        \n",
        "      decoder_output = torch.zeros(max_length, latent_dim, device=device)\n",
        "\n",
        "      fc3 = nn.Linear(encoder1.hidden_size, latent_dim)\n",
        "      fc3.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for ei in range(input_length):\n",
        "              encoder_output, encoder_hidden = encoder1(input_tensor[ei], encoder_hidden)\n",
        "              encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "      encoder_outputs = fc3(encoder_outputs)\n",
        "      c_optimizer.zero_grad()\n",
        "\n",
        "      noise = torch.from_numpy(np.random.normal(0, 1, (input_length, latent_dim))).float()\n",
        "      noise = noise.to(device)\n",
        "\n",
        "      z_fake = generator(noise)        \n",
        "      z_fake.to(device)\n",
        "\n",
        "      real_score = critic(encoder_outputs)\n",
        "      fake_score = critic(z_fake)\n",
        "      grad_penalty = compute_grad_penalty(critic, encoder_outputs.data, z_fake.data)\n",
        "      \n",
        "      c_loss = -torch.mean(real_score) + torch.mean(fake_score) + gp_lambda * grad_penalty\n",
        "      c_train_loss += c_loss.item()\n",
        "      c_loss.backward()\n",
        "      c_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      # train generator\n",
        "      if i % n_critic == 0:\n",
        "          g_batches += 1\n",
        "          g_optimizer.zero_grad()\n",
        "          fake_score = critic(generator(noise))\n",
        "          g_loss = -torch.mean(fake_score)\n",
        "          g_train_loss += g_loss.item()\n",
        "          g_loss.backward()\n",
        "          g_optimizer.step()\n",
        "\n",
        "      if interval > 0 and i % interval == 0:\n",
        "          print('Epoch: {} | Batch: {}/{} ({:.0f}%) | G Loss: {:.6f} | C Loss: {:.6f}'.format(\n",
        "              epoch, batch_size * i, len(pairs),\n",
        "                      100. * (batch_size * i) / len(pairs),\n",
        "              g_loss.item(), c_loss.item()\n",
        "          ))\n",
        "\n",
        "    print(\"End of loop ====>>>>>\")\n",
        "    g_train_loss /= g_batches\n",
        "    c_train_loss /= len(pairs)\n",
        "    print('* (Train) Epoch: {} | G Loss: {:.4f} | C Loss: {:.4f}'.format(\n",
        "        epoch, g_train_loss, c_train_loss\n",
        "    ))\n",
        "    return (g_train_loss, c_train_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehYDEKRmcXUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "8b2cdb9d-4b3d-4661-810a-6596ddd0abdc"
      },
      "source": [
        "best_loss = np.inf\n",
        "epochs = 10\n",
        "for epoch in range(1, epochs + 1):\n",
        "    g_loss, c_loss = train(epoch)\n",
        "    loss = g_loss + c_loss\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        print('* Saved')\n",
        "        torch.save(generator.state_dict(), '/content/drive/My Drive/translate/generator_question_answer_ubuntu.th')\n",
        "        torch.save(critic.state_dict(), '/content/drive/My Drive/translate/critic_question_answer_ubuntu.th')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Batch: 0/2738 (0%) | G Loss: 0.200958 | C Loss: 6350.701660\n",
            "Epoch: 1 | Batch: 1000/2738 (37%) | G Loss: -0.079259 | C Loss: 5.721465\n",
            "Epoch: 1 | Batch: 2000/2738 (73%) | G Loss: -0.058214 | C Loss: 0.532286\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 1 | G Loss: -0.0450 | C Loss: 28.2302\n",
            "* Saved\n",
            "Epoch: 2 | Batch: 0/2738 (0%) | G Loss: 0.047730 | C Loss: 6641.666016\n",
            "Epoch: 2 | Batch: 1000/2738 (37%) | G Loss: -0.090551 | C Loss: 6.843675\n",
            "Epoch: 2 | Batch: 2000/2738 (73%) | G Loss: -0.057204 | C Loss: 2.255315\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 2 | G Loss: -0.0557 | C Loss: 28.6908\n",
            "Epoch: 3 | Batch: 0/2738 (0%) | G Loss: -0.170604 | C Loss: 6781.441895\n",
            "Epoch: 3 | Batch: 1000/2738 (37%) | G Loss: -0.114652 | C Loss: 4.123209\n",
            "Epoch: 3 | Batch: 2000/2738 (73%) | G Loss: -0.083017 | C Loss: 1.358951\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 3 | G Loss: -0.0847 | C Loss: 31.0343\n",
            "Epoch: 4 | Batch: 0/2738 (0%) | G Loss: -0.007825 | C Loss: 6561.869629\n",
            "Epoch: 4 | Batch: 1000/2738 (37%) | G Loss: -0.104540 | C Loss: 3.942480\n",
            "Epoch: 4 | Batch: 2000/2738 (73%) | G Loss: -0.096259 | C Loss: 0.460311\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 4 | G Loss: -0.0801 | C Loss: 28.2464\n",
            "* Saved\n",
            "Epoch: 5 | Batch: 0/2738 (0%) | G Loss: 0.038305 | C Loss: 7777.271484\n",
            "Epoch: 5 | Batch: 1000/2738 (37%) | G Loss: -0.074856 | C Loss: 3.824920\n",
            "Epoch: 5 | Batch: 2000/2738 (73%) | G Loss: -0.067440 | C Loss: 0.913260\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 5 | G Loss: -0.0556 | C Loss: 31.7152\n",
            "Epoch: 6 | Batch: 0/2738 (0%) | G Loss: -0.030259 | C Loss: 5519.488281\n",
            "Epoch: 6 | Batch: 1000/2738 (37%) | G Loss: -0.076097 | C Loss: 6.127678\n",
            "Epoch: 6 | Batch: 2000/2738 (73%) | G Loss: -0.075339 | C Loss: 1.005463\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 6 | G Loss: -0.0656 | C Loss: 25.6207\n",
            "* Saved\n",
            "Epoch: 7 | Batch: 0/2738 (0%) | G Loss: 0.047143 | C Loss: 6491.707031\n",
            "Epoch: 7 | Batch: 1000/2738 (37%) | G Loss: -0.091582 | C Loss: 1.008895\n",
            "Epoch: 7 | Batch: 2000/2738 (73%) | G Loss: -0.084058 | C Loss: 0.816917\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 7 | G Loss: -0.0714 | C Loss: 28.9234\n",
            "Epoch: 8 | Batch: 0/2738 (0%) | G Loss: -0.021361 | C Loss: 5706.249023\n",
            "Epoch: 8 | Batch: 1000/2738 (37%) | G Loss: -0.074252 | C Loss: 7.765429\n",
            "Epoch: 8 | Batch: 2000/2738 (73%) | G Loss: -0.091901 | C Loss: 1.501570\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 8 | G Loss: -0.0695 | C Loss: 26.3055\n",
            "Epoch: 9 | Batch: 0/2738 (0%) | G Loss: 0.035625 | C Loss: 6652.611816\n",
            "Epoch: 9 | Batch: 1000/2738 (37%) | G Loss: -0.094490 | C Loss: 5.150913\n",
            "Epoch: 9 | Batch: 2000/2738 (73%) | G Loss: -0.066172 | C Loss: 1.346233\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 9 | G Loss: -0.0638 | C Loss: 31.8798\n",
            "Epoch: 10 | Batch: 0/2738 (0%) | G Loss: -0.022632 | C Loss: 6544.214355\n",
            "Epoch: 10 | Batch: 1000/2738 (37%) | G Loss: -0.092082 | C Loss: 5.704449\n",
            "Epoch: 10 | Batch: 2000/2738 (73%) | G Loss: -0.070609 | C Loss: 0.542634\n",
            "End of loop ====>>>>>\n",
            "* (Train) Epoch: 10 | G Loss: -0.0632 | C Loss: 28.4355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh0v7sAF5fqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testGan(encoder, decoder, sentence, max_length=10):\n",
        "    with torch.no_grad():\n",
        "        #print(\"1\")\n",
        "        input_length = sentence.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "        encoder_outputs[0: input_length,:] = sentence\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "        \n",
        "        #print(\"2\")\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            #print(\"3\")\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOhbuXuo5mnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import randint\n",
        "\n",
        "def evaluateGAN(encoder, decoder, encoder_outputs, n=100):\n",
        "    latent_dim = 256\n",
        "    n_layers = 20\n",
        "    block_dim = 256\n",
        "    output_sentence_list = []\n",
        "    while(len(output_sentence_list) < 100):\n",
        "        generator = Generator(n_layers, block_dim)\n",
        "        generator.eval()\n",
        "        generator.load_state_dict(torch.load('/content/drive/My Drive/translate/generator_question_answer_ubuntu.th', map_location='cpu'))\n",
        "        input_tensor = [[85],[86],[109],[1362],[2],[6],[1]]\n",
        "        input_tensor = torch.from_numpy(np.asarray(input_tensor)).float()\n",
        "        noise = torch.from_numpy(np.random.normal(loc = 0.7, scale = 1, size = (10,256)))\n",
        "        \n",
        "        #noise = torch.from_numpy(input_tensor).float()\n",
        "        #z = generator(noise)\n",
        "        #print(\"Z shape=\",z.shape)\n",
        "        #print(\"z=\", z)\n",
        "        pair = random.choice(pairs)\n",
        "        #print('>', pair[0])\n",
        "        #print('=', pair[1])\n",
        "        #print(noise)\n",
        "        #output_words, attentions = testGan(encoder, decoder, encoder_outputs[i])\n",
        "        output_words, attentions = testGan(encoder, decoder, noise)\n",
        "        #output_words, attentions = testGan(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "\n",
        "        if(output_sentence not in output_sentence_list and len(output_sentence.split(\" \"))>6):\n",
        "            output_sentence_list.append(output_sentence)\n",
        "            print('<', output_sentence)\n",
        "            print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLSTvU_o5m6A",
        "colab_type": "code",
        "outputId": "09346b1e-2582-4e38-8df8-87422eb6519f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluateGAN(encoder1,attn_decoder1, encoder_outputs)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< i m in the same thing ? <EOS>\n",
            "\n",
            "< i m using linux right now <EOS>\n",
            "\n",
            "< i m in the same thing <EOS>\n",
            "\n",
            "< i m in to do . <EOS>\n",
            "\n",
            "< i m a linux newbie . <EOS>\n",
            "\n",
            "< i m in to do now <EOS>\n",
            "\n",
            "< i m in the wrong problem <EOS>\n",
            "\n",
            "< i m in to a problem <EOS>\n",
            "\n",
            "< i m in the same i <EOS>\n",
            "\n",
            "< i m in the same thing . <EOS>\n",
            "\n",
            "< i m using linux right ? <EOS>\n",
            "\n",
            "< i m an to do . <EOS>\n",
            "\n",
            "< i m in the wrong channel <EOS>\n",
            "\n",
            "< i m sorry to do . <EOS>\n",
            "\n",
            "< i m from to play this <EOS>\n",
            "\n",
            "< i m in the ntfs  <EOS>\n",
            "\n",
            "< i m in the wrong of <EOS>\n",
            "\n",
            "< i m using xchat do now <EOS>\n",
            "\n",
            "< i m in the same i think <EOS>\n",
            "\n",
            "< i m in the same . <EOS>\n",
            "\n",
            "< i m using linux do ? <EOS>\n",
            "\n",
            "< i m in the same i folder <EOS>\n",
            "\n",
            "< i m using xchat my desktop <EOS>\n",
            "\n",
            "< i m in to play this <EOS>\n",
            "\n",
            "< i m in the same i do <EOS>\n",
            "\n",
            "< i m in the same i thing <EOS>\n",
            "\n",
            "< i m an linux right now <EOS>\n",
            "\n",
            "< i m in network right now <EOS>\n",
            "\n",
            "< i m here to play . <EOS>\n",
            "\n",
            "< i m an to play this <EOS>\n",
            "\n",
            "< i m in the as havn <EOS>\n",
            "\n",
            "< i m in the same i do ? <EOS>\n",
            "\n",
            "< i m using linux right now  <EOS>\n",
            "\n",
            "< i m a bot with me <EOS>\n",
            "\n",
            "< i m in the same  <EOS>\n",
            "\n",
            "< i m in the wrong version <EOS>\n",
            "\n",
            "< i m in the wrong irc <EOS>\n",
            "\n",
            "< i m in the wrong that <EOS>\n",
            "\n",
            "< i m here to do . <EOS>\n",
            "\n",
            "< i m from to do now <EOS>\n",
            "\n",
            "< i m in to play . <EOS>\n",
            "\n",
            "< i m in the wrong my <EOS>\n",
            "\n",
            "< i m asking to do . <EOS>\n",
            "\n",
            "< i m sorry to do now <EOS>\n",
            "\n",
            "< i m in to linux . <EOS>\n",
            "\n",
            "< i m in the same .  <EOS>\n",
            "\n",
            "< i m using linux do . <EOS>\n",
            "\n",
            "< i m a love with me <EOS>\n",
            "\n",
            "< i m using xchat right now <EOS>\n",
            "\n",
            "< i m sorry to linux . <EOS>\n",
            "\n",
            "< i m in the same now  <EOS>\n",
            "\n",
            "< i m an linux do . <EOS>\n",
            "\n",
            "< i m using xchat gnome . <EOS>\n",
            "\n",
            "< i m asking to do this <EOS>\n",
            "\n",
            "< i m a to play this <EOS>\n",
            "\n",
            "< i m using xchat do ? <EOS>\n",
            "\n",
            "< i m in my as well <EOS>\n",
            "\n",
            "< i m a to do my ubuntu <EOS>\n",
            "\n",
            "< i m sorry to do my <EOS>\n",
            "\n",
            "< i m in the sh  <EOS>\n",
            "\n",
            "< i m from to do . <EOS>\n",
            "\n",
            "< i m in the same i ? <EOS>\n",
            "\n",
            "< i m upgrading to do . <EOS>\n",
            "\n",
            "< i m in the same problem <EOS>\n",
            "\n",
            "< i m in the right now <EOS>\n",
            "\n",
            "< i m sorry to do this <EOS>\n",
            "\n",
            "< i m a linux with my <EOS>\n",
            "\n",
            "< i m in to do this <EOS>\n",
            "\n",
            "< i m doing to play this <EOS>\n",
            "\n",
            "< i m in the wrong thing . <EOS>\n",
            "\n",
            "< i m in to gnome . <EOS>\n",
            "\n",
            "< i m a linux noob  <EOS>\n",
            "\n",
            "< i m from to play . <EOS>\n",
            "\n",
            "< i m in the wrong it <EOS>\n",
            "\n",
            "< i m in the ntfs problem <EOS>\n",
            "\n",
            "< i m a linux to this <EOS>\n",
            "\n",
            "< i m using the ntfs  <EOS>\n",
            "\n",
            "< i m in the wrong kernel <EOS>\n",
            "\n",
            "< i m in the wrong .  <EOS>\n",
            "\n",
            "< i m sorry to play this <EOS>\n",
            "\n",
            "< i m in to this . <EOS>\n",
            "\n",
            "< i m using to do now <EOS>\n",
            "\n",
            "< i m a to reinstall . <EOS>\n",
            "\n",
            "< i m a linux right now <EOS>\n",
            "\n",
            "< i m using the same thing <EOS>\n",
            "\n",
            "< i m in to a my <EOS>\n",
            "\n",
            "< i m on to do . <EOS>\n",
            "\n",
            "< i m a console with my <EOS>\n",
            "\n",
            "< i m a beginner with my <EOS>\n",
            "\n",
            "< i m sorry to do my desktop <EOS>\n",
            "\n",
            "< i m using xchat do . <EOS>\n",
            "\n",
            "< i m too to do . <EOS>\n",
            "\n",
            "< i m a bot with my <EOS>\n",
            "\n",
            "< i m in the same but i <EOS>\n",
            "\n",
            "< i m in the wrong my beagleboard <EOS>\n",
            "\n",
            "< i m a to this . <EOS>\n",
            "\n",
            "< i m sorry to do that <EOS>\n",
            "\n",
            "< i m a linux with it <EOS>\n",
            "\n",
            "< i m in the wrong thing <EOS>\n",
            "\n",
            "< i m using xchat my question <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX9QqyHM5mvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnYnq-eB5mtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}